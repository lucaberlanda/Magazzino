{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='font-family:Inter'> Statistics Notes\n",
    "\n",
    "## <span style='font-family:Inter'> Inequalities, Convergence and WLLN\n",
    "\n",
    "### <span style='font-family:Inter'> Markov Inequality\n",
    "\n",
    "Given a **positive rv**, finite first moment ($E(X) < ∞$); it states that:\n",
    "\n",
    "$${\\displaystyle \\operatorname {P} (X\\geq a)\\leq {\\frac {\\operatorname {E} (X)}{a}}.}$$\n",
    "\n",
    "i.e. the probability of a rv of being greater than the constant $a$ should be less or equal to the expected value divided by $a$. _The intuition is that, if this doesn't hold, the expected value couldn't have the value it takes._ Here the [**video**](https://www.youtube.com/watch?v=uh-v7LchsxU). Useful only on the upper bound. Remembering that the rv should be positive greatly helps in the intuitive explanation of the inequality.\n",
    "\n",
    "\n",
    "### <span style='font-family:Inter'> Chebyshev Inequality\n",
    "If I also know the variance of the rv, and I know that it is not infinite, I can get a better bound compared to Markov Inequality.\n",
    "\n",
    "$$\\Pr(|X-\\mu |\\geq k\\sigma )\\leq {\\frac {1}{k^{2}}}$$\n",
    "\n",
    "The proof trasforms the left argument by squaring $(x - \\mu)$ and $k$, then simply applies Markov Inequality. $X_1, ...., X_n$ i.i.d. then\n",
    "\n",
    "$$\n",
    "\\displaystyle{M_n = \\frac{X_1 + ... + X_n}{n}} \\rightarrow E[X] \\\\\n",
    "\\displaystyle{E[M_n] = \\frac{E[X_1 + ... + X_n]}{n}} = \\frac{n\\mu^2}{n} = \\mu\\\\\n",
    "\\displaystyle{ \\operatorname{Var}[M_n] = \\frac{\\operatorname{Var}(X_1 + ... + X_n)}{n^2}} = \\frac{n\\sigma^2}{n^2} = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "\n",
    "Appling Chebyshev, we get:\n",
    "\n",
    "$$\n",
    "\\displaystyle{P(|M_n - \\mu| \\geq \\epsilon) \\leq \\frac{\\operatorname{Var}(M_n)}{\\epsilon^2}= \\frac{\\sigma^2}{n\\epsilon^2} = \\frac{\\sigma^2}{n}}\n",
    "$$\n",
    "\n",
    "\n",
    "which goes to zero for a fixed $\\epsilon$ and for $n$ that goes to infinity\n",
    "\n",
    "### Convergence in probability\n",
    "\n",
    "1. conjecture of limit\n",
    "2. compute the probability of being $\\epsilon$ away from conjectured limit\n",
    "\n",
    "### Chernoff bound\n",
    "\n",
    "Better bound\n",
    "\n",
    "### Hoeffding's Inequality\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle \\displaystyle \\mathbf{P}\\left(\\left|\\overline{X}_ n-\\mathbb E[X]\\right|\\geq \\epsilon \\right) \\leq 2 \\exp \\left(-\\frac{2n\\epsilon ^2}{(b-a)^2}\\right)\\qquad \\text {for all }\\epsilon >0\n",
    "$$\n",
    "\n",
    "### Law of large numbers (LLN)\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "The law of large numbers guarantees that the realized mean of a random variables converges to the real mean as $n \\to \\infty$\n",
    "</div>\n",
    "\n",
    "$$\\overline{X_n} \\xrightarrow{P} \\mu, \\quad n \\to \\infty $$\n",
    "\n",
    "Finitness of variance is not required, even if it accelerated the convergence.\n",
    "\n",
    "\n",
    "### Central Limit theorem\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "Central limit theorem regards the <strong>distribution</strong> of the mean of a random variable\n",
    "</div>\n",
    "\n",
    "given iid rvs:\n",
    "\n",
    "- $X_1, \\dots , X_n$ are i.i.d.;\n",
    "\n",
    "- $\\mathbb {E}[X_1] = \\mu < \\infty ,$ and $\\text {Var}(X_1) = \\sigma ^2 < \\infty$\n",
    "\n",
    "then\n",
    "\n",
    "$${\\displaystyle Z_n = \\frac{X_1 + ... + X_n - \\mu n}{\\sigma\\sqrt n}}$$\n",
    "\n",
    "  $${\\displaystyle \\lim _{n\\to \\infty }\\Pr(Z_n \\leq z) = \\Phi (z)}$$\n",
    "\n",
    "  Stated in a different way:\n",
    "\n",
    "  $$\\sqrt{n}\\left( \\overline{X} _n - \\mu \\right) = \\sqrt{n}\\left( \\left(\\frac{1}{n} \\sum_ {i = 1}^ n X_ i\\right) - \\mu \\right) \\xrightarrow [n \\to \\infty ]{(d)} Z,$$\n",
    "\n",
    "  where $Z$ is a normal random variable with mean $0$ and variance $\\sigma^2$.\n",
    "\n",
    "  c) Dependence will not make a difference because the definition of convergence in probability involves probabilities of the form $P(|Yn−a|≥ϵ)$. These probabilities are completely determined by the marginal distributions of the random variables Yn , and these marginal distributions are the same as for the sequence Xn.\n",
    "\n",
    "\n",
    "\"Consistent estimator\" is defined at 3:21 of the previous lecture. In the context of statistics, the adjective \"consistent\" means that the estimator $Θ^n$ converges in probability towards $θ$ when $n→+∞$ for every possible value of the parameter $θ$.\n",
    "\n",
    "Consistent estimator: converges to real one as n goes inf Unbiased: expectation is equal to the real value\n",
    "\n",
    "C.I.\n",
    "\n",
    "Use upper bound use sample means estimate use sample variance estimate\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli and Poisson Processes\n",
    "\n",
    "Poisson process foresees arrivals over time in an **uncoordinated** manner. The poisson distribution is:\n",
    "\n",
    "obtained taking the limit of the binomial process having $n$ that goes to infinity and $p$ that goes to zero so that $np$ stays constant.\n",
    "\n",
    "$$\\displaystyle{P(N=n)={\\frac {\\lambda ^{n}}{n!}}e^{-\\lambda }}$$\n",
    "\n",
    "The probability of an arrival over a time period of time $\\tau$ is equal to:\n",
    "\n",
    "$$\\tau \\lambda$$\n",
    "\n",
    "and, over an interval of lenght $\\tau$:\n",
    "\n",
    "$$\\displaystyle{P(N_\\tau=n)={\\frac {(\\lambda \\tau) ^{n}}{n!}}e^{-(\\lambda \\tau)}}$$\n",
    "\n",
    "$$\\displaystyle{E[N_\\tau]= \\lambda \\tau}$$\n",
    "\n",
    "$$\\displaystyle{Var[N_\\tau]= \\lambda \\tau}$$\n",
    "\n",
    "- **Time until first arrival**: the CDF and PDF of the time until the first arrival are respectively:\n",
    "\n",
    "  $$\\displaystyle{F(x)=1 - e^{-\\lambda \\tau}}$$\n",
    "\n",
    "  $$\\displaystyle{f(x)=\\lambda e^{-\\lambda \\tau}}$$\n",
    "\n",
    "  The last one is an Exponential distribution, which is memoryless. The time until the $k$th arrival is instead (**Erlang Distribution**):\n",
    "\n",
    "  $$f_{Y_k}={\\lambda^{k}y^{{k-1}}e^{{-\\lambda y}} \\over (k-1)!}$$\n",
    "\n",
    "  $$E_{Y_k}= \\frac{k}{\\lambda}$$\n",
    "\n",
    "  $$Var(Y_k)=\\frac{k}{\\lambda^2}$$\n",
    "\n",
    "Also interarrival times are exponential, so $Y_k$\n",
    "\n",
    "The sum of two independent Poisson processes (**merged process**) is Poisson and the parameter is the sum of the parameters.\n",
    "\n",
    "In a merged process, the probability that a particular arrival came from the first stream is equal to:\n",
    "\n",
    "$$\\frac{\\lambda_1}{(\\lambda_2 + \\lambda_1)}$$\n",
    "\n",
    "where $\\lambda_1$ and $\\lambda_2$ are the parameters of the first and of the second stream respectively.\n",
    "\n",
    "- Random Incidence\n",
    "\n",
    "- The **Pascal random variable** is an extension of the geometric random variable. It describes the number of trials until the kth success, which is why it is sometimes called the \"kth-order interarrival time for a Bernoulli process.\"\n",
    "\n",
    "- for an exponential distribution:\n",
    "\n",
    "  $$E[X^2] = 2 / \\lambda^2$$\n",
    "\n",
    "# Statistics\n",
    "\n",
    "Trinity:\n",
    "\n",
    "- Estimation\n",
    "- Confidence intervals\n",
    "- Hypoteshis testing\n",
    "\n",
    "More on Central Limit Theorem\n",
    "\n",
    "- **Law of Large Numbers (LLN)**\n",
    "\n",
    "- **PDF**\n",
    "\n",
    "  $${\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {2\\pi }}}e^{-{\\frac {1}{2}}x^{2}}}$$\n",
    "\n",
    "  $${\\displaystyle \\varphi (x)={\\frac {1}{\\sigma \\sqrt {2\\pi }}}e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}}}$$\n",
    "\n",
    "- **Invariant Under Affine Transformation**: Given a Gaussian $X\\sim \\mathcal{N}(\\mu, \\sigma^2)$, $aX + b$ is still a Gaussian with mean $a\\mu + b$ and variance $a^2\\sigma^2$.\n",
    "\n",
    "- **Standardization**:\n",
    "\n",
    "  $$\n",
    "  Z = \\frac{X - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)%%\n",
    "  $$\n",
    "\n",
    "- **Identifiability**: Recall that the parameter $θ$ is identifiable if the map $θ↦P_θ$ is injective. Here, the notation $θ↦P_θ$ denotes a function that takes as input $θ∈Θ$ and outputs a probability distribution $P_θ$. In other words, if $θ≠θ^′$ (and both in $Θ$), then $P_θ≠P_{θ^′}$\n",
    "\n",
    "- **Symmetry**: if $X\\sim \\mathcal{N}(\\mu, \\sigma^2)$ then $-X\\sim \\mathcal{N}(\\mu, \\sigma^2)$\n",
    "\n",
    "- **XXX**: Let $X_1,...,X_n$ be i.i.d. random variables with mean $μ$ and variance $σ^2$. Denote the sample mean by $\\overline{X}_ n = (X_1 + ... + X_n)/n$. Assume that $n$ is large enough that the central limit theorem (clt) holds. The following random variable $Z$ has approximate distribution $X\\sim \\mathcal{N}(0, 1)$:\n",
    "\n",
    "$$\\displaystyle Z =\\frac{\\overline{X}_ n-\\mu }{\\sqrt{\\sigma ^2/n}}$$\n",
    "\n",
    "$$\\displaystyle Z =\\sqrt{n}\\frac{\\overline{X}_ n-\\mu }{\\sigma }$$\n",
    "\n",
    "- The **quantile** of order $1−α$ of a variable $X$, denoted by $q_α$ (specific to a particular $X$), is the number such that $P(X≤q_α)=1−α$.\n",
    "\n",
    "- **Convergence**: from Strong to Weak we have:\n",
    "\n",
    "  - almost sure convergence\n",
    "  - convergence in probability\n",
    "  - convergence in distribution\n",
    "\n",
    "- Convergence in probability does not imply convergence in expectation, e.g. given $T_n$ as\n",
    "\n",
    "  $$\\displaystyle \\displaystyle \\mathbf{P}(T_ n=0) = 1 - \\frac{1}{n}$$\n",
    "\n",
    "  $$\\displaystyle \\displaystyle \\mathbf{P}(T_ n=2^n) = \\frac{1}{n}$$\n",
    "\n",
    "  Then\n",
    "\n",
    "  $$\\displaystyle \\displaystyle \\mathbf{P}\\left(\\left|T_ n-0\\right|>\\epsilon \\right)=\\frac{1}{n}\\longrightarrow 0.$$\n",
    "\n",
    "  Which implies convergence in probability. But:\n",
    "\n",
    "  $$\\displaystyle E[T_n] = \\frac{2^ n}{n}\\longrightarrow \\infty$$\n",
    "\n",
    "- Slusky Theorem: you can sum or multiply a rv that converges in distribution with a rv that converges in probability only if the one which converges in probability is a constant\n",
    "\n",
    "- def of an **estimator**: statitics that does not depend on my unknown parameter\n",
    "\n",
    "- **Sample Space**: where my observations lay; **Parameter Space**: where my parameters lay\n",
    "\n",
    "- Note that the sample space of X is not unique. For example, if X∼Ber(p) , then both {0,1} and R can serve as a sample space. However, in general, we associate a random variable with its smallest possible sample space (which would be {0,1} if X∼Ber(p) ).\n",
    "\n",
    "- parametric models: parameters have finite dimensions, nonparametric models: parameters have infinite dimensions,\n",
    "\n",
    "A model is **well-specified** if $\\exists \\theta s.t. P = P_\\theta$, i.e. that for some $\\theta$, the distribution of th unknown is the right distribution.\n",
    "\n",
    "Linear regression model: $(X_1, Y_1),\\ldots , (X_ n,Y _n) \\in \\mathbb {R}^ d \\times \\mathbb {R}$ are i.i.d from the linear regression model $Y_ i=\\beta ^\\top X _i + \\varepsilon_ i, \\quad \\varepsilon _i \\stackrel{iid}{\\sim } \\mathcal{N}(0,1)$ for an unknown $β∈R^d$ and $X_ i \\sim \\mathcal{N} _d(0,I_ d)$ independent of εi .\n",
    "\n",
    "- **Jensen's Inequality**: make an example\n",
    "\n",
    "- **Bias of an estimator**: given the estimator of $\\theta$, i.e. $\\hat{\\theta_n}$, the bias of the estimator is defined as:\n",
    "\n",
    "  $$\\mathbb E[\\hat{\\theta }_ n] - \\theta .$$\n",
    "\n",
    "- **Variance of an estimator**\n",
    "\n",
    "- **Quadratic risk** is defined as:\n",
    "\n",
    "  $$\\mathbb E[(\\hat{\\theta }_ n - \\theta )^2].$$\n",
    "\n",
    "  Minimizing that means minimizing both variance and bias of the estimator. Indeed, expanding the formula we get that the quadratic risk is equal to the bias squared plus the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='font-family:Inter'> Confidence Intervals\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "In statistics, a confidence interval (CI) is a type of estimate computed from the <strong>observed data</strong>;  Because the CI is an estimate, it is computed from a sample.\n",
    "</div>\n",
    "\n",
    "it's random, depend on data, but they do not depend on $\\theta$. Defined as \"probability that Confidence interval contains $\\theta$\".\n",
    "\n",
    "**Example**\n",
    "\n",
    "You have the estimator $\\bar{R}_n$ as defined above. You want the probability of the distance between this estimator and the true parameter $p$ be more then $x$ with low probability (probability $α$), i.e.:\n",
    "\n",
    "$$P(|\\bar{R}_n-p|\\ge x)=\\alpha$$\n",
    "\n",
    "So you first normalize the variable:\n",
    "\n",
    "$$P(\\frac{\\sqrt{n} |\\bar{R}_n-p|}{\\sqrt{p(1-p)}}\\ge \\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}})=\\alpha$$\n",
    "\n",
    "but notice that the random variable converges to a standard normal, so:\n",
    "\n",
    "$$P(|Z|\\ge \\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}})=\\alpha$$\n",
    "\n",
    "that follows because of convergence in distribution that is guaranteed by the Central Limit Theorem.\n",
    "\n",
    "$$P\\Bigg( \\frac{\\sqrt{n} |\\bar R_n - p| }{\\sqrt{p(1-p)}} \\ge \\frac{\\sqrt{n} x }{\\sqrt{p(1-p)}} \\Bigg) \\approx P\\Bigg( |Z| \\ge \\frac{\\sqrt{n} x }{\\sqrt{p(1-p)}} \\Bigg)$$\n",
    "\n",
    "You will have this:\n",
    "\n",
    "$$2*P(Z\\ge \\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}})=\\alpha$$\n",
    "\n",
    "or using the complement:\n",
    "\n",
    "$$2*\\left(1-P(Z\\lt \\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}}) \\right)=\\alpha$$\n",
    "\n",
    "using the CDF:\n",
    "\n",
    "$$2*\\left(1-\\Phi(\\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}}) \\right)=\\alpha$$\n",
    "\n",
    "That is equivalent to:\n",
    "\n",
    "$$\\Phi(\\frac{\\sqrt{n}x}{\\sqrt{p(1-p)}}) =1-\\alpha/2$$\n",
    "\n",
    "Inverting the CDF\n",
    "\n",
    "$$x=\\frac{\\sqrt{p(1-p)}\\Phi^{-1}(1-\\alpha/2)}{\\sqrt{n}}$$\n",
    "\n",
    "but notice that the inverse is just the percentile of $α/2$, so:\n",
    "\n",
    "$$x=\\frac{\\sqrt{p(1-p)}q_{\\alpha/2}}{\\sqrt{n}}$$\n",
    "\n",
    "three solutions then:\n",
    "\n",
    "- Conservative bound\n",
    "- Solving Quadratic Equation for $p$\n",
    "- Plug-in\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Asymptotic Confidence Interval\n",
    "\n",
    "(Slide 19/61 of Chapter 2) - An interval $I$ (possibly random) whose boundaries do not depend on $θ$ (the unknown parameter) and such that\n",
    "\n",
    "$$\\lim_{n \\to \\infty} \\mathbf{P}_\\theta[\\theta \\in \\mathcal{I}] \\ge 1-\\alpha, \\forall \\theta \\in \\Theta$$\n",
    "\n",
    "is called an asymptotic confidence interval of level $1−α$. Note that the above does not necessarily mean that the interval must depend upon $n$. Nor does it mean that the interval is required to be random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Asymptotic Variance of an Estimator\n",
    "\n",
    "The asymptotic variance of an estimator $\\widehat{θ}$ for a parameter $θ$ is defined as $V(\\widehat{\\theta})$, if\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\theta } - \\theta ) \\xrightarrow [n \\to \\infty ]{\\mathrm{(D)}} \\mathcal{N}(0, V(\\widehat{\\theta }))$$\n",
    "\n",
    "### Delta Method\n",
    "\n",
    "Roughly, if there is a sequence of random variables $X_n$ satisfying\n",
    "\n",
    "$$\n",
    "{{\\sqrt {n}}[X_{n}-\\theta ]\\,{\\xrightarrow {D}}\\,{\\mathcal {N}}(0,\\sigma ^{2})},\n",
    "$$\n",
    "where $θ$ and $σ^2$ are finite valued constants and ${\\displaystyle {\\xrightarrow {D}}}$ denotes convergence in distribution, then\n",
    "\n",
    "$$\n",
    "{\\displaystyle {{\\sqrt {n}}[g(X_{n})-g(\\theta )]\\,{\\xrightarrow {D}}\\,{\\mathcal {N}}(0,\\sigma ^{2}\\cdot [g'(\\theta )]^{2})}}\\\\ \\displaystyle \\mathcal{N}\\left(0,\\left(g'\\left(\\frac{1}{\\lambda }\\right)\\right)^2 \\left(\\frac{1}{\\lambda ^2}\\right)\\right)\n",
    "$$\n",
    "Warning (example on the ): It's very important that we apply g′ to the value 1/λ , and not λ . We start with a consistent estimator, namely X¯¯¯¯n , whose limit is E[X]=1/λ , and the Delta method asks us to apply g′ to the limit of that consistent estimator. Be careful about this, as it is a common source of errors. The Delta method states that continuously differentiable function applied to an asymptotically normal sequence of random variables is again asymptotically normal.\n",
    "\n",
    "- Two Sample test, e.g. difference between two averages\n",
    "- One Sample test, e.g. difference vis a vis a benchmark\n",
    "\n",
    "### Hypothesis Testing\n",
    "\n",
    "- $H_0$: $\\theta \\in \\Theta_0$\n",
    "- $H_1$: $\\theta \\in \\Theta_1$\n",
    "\n",
    "Always looking for evidence against $H_0$, never evidence in favor of it. Remark: Regardless of the data, our conclusion will never be to accept the null. On observing the data, we will either reject the null in favor of the alternative OR we will fail to reject the null. In the latter case, we are not claiming that the null is true, rather we are stating that the data does not provide us with enough evidence to refute the null hypothesis.\n",
    "\n",
    "- **Test Statistics**:\n",
    "\n",
    "- **Type 1 and Type 2 Error**: In this example, let's say that the jury makes a type 1 error if the suspect satisfies H0 while the jury rules in favor of H1 . Let's say the jury makes a type 2 error if the suspect satisfies H1 while the jury rules in favor of H0.\n",
    "\n",
    "  - **Type 1**: [...]\n",
    "\n",
    "  - **Type 2**: where $Pθ(ψ_n=0)$ is the probability of the event $ψ_n=0$ under the probability distribution $P_θ$ when $θ∈Θ_1$ , i.e. the probability of not rejecting $H_0$ when $H_1$ is true\n",
    "\n",
    "- **Power of a test**: is defined as\n",
    "\n",
    "  $$\\pi _{\\psi_ n} = \\inf _{\\theta \\in \\Theta _1} (1 - \\beta_ {\\psi _ n}(\\theta))$$\n",
    "\n",
    "  _That_ is, the power of a test is the lowest possible value that one minus the type 2 error can take, given that $\\theta$ belongs to $\\Theta_1$, i.e. given that $H_0$ should be refused.\n",
    "\n",
    "- A test $ψ$ has level $α$ if\n",
    "\n",
    "  $$\\displaystyle \\alpha \\geq \\alpha _{\\psi }(\\theta )\\qquad \\text {for all }\\, \\theta \\in \\Theta _0$$\n",
    "\n",
    "  _where_ $\\alpha _{\\psi }=\\mathbf{P}_\\theta (\\psi =1)$ is the type 1 error. We will often use the word \"level\" to mean the \"smallest\" such level, i.e. the least upper bound of the type 1 error, defined as follows:\n",
    "\n",
    "  $$\\displaystyle \\text {sup}_{\\theta \\in \\Theta _0} \\alpha_ {\\psi }(\\theta )$$\n",
    "\n",
    "- Determine the smallest threshold $C$ such that the test $ψ_{n,C}$ has level $α$, i.e. the type 1 error should be equal to $\\alpha$ given that $\\theta \\in \\Theta_0$\n",
    "\n",
    "### Total Variation Distance\n",
    "\n",
    "It is defined as (for discrete and continuous rvs respectively):\n",
    "\n",
    "$$\\text {TV}(\\mathbf{P}_{\\theta }, \\mathbf{P}_{\\theta '})={\\max _{A \\subset E}}\\, \\big |\\mathbf{P}_{\\theta }(A)-\\mathbf{P}_{\\theta '}(A)\\big |\\,$$\n",
    "\n",
    "$$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = \\frac{1}{2} \\, \\sum _{x \\in E} |f(x) - g(x)|.$$\n",
    "\n",
    "$$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = \\frac{1}{2} \\, {\\color{blue}{\\int }} _{x \\in E} |f(x) - g(x)|$$\n",
    "\n",
    "Informally_, this is the largest possible difference between the probabilities that the two probability distributions can assign to the same event. Note that the distance between two distributions only depends on the distributions themselves and not their relation to each other (the joint distribution). This is why assuming $X$ and $Y$ are independent (or not) does not affect the total variation distance.\n",
    "\n",
    "- $d(P,Q)=d(Q,P)$ (symmetric)\n",
    "- $d(P,Q)≥0$ (nonnegative)\n",
    "- $d(P,Q)=0⟺P=Q$ (definite)\n",
    "- $d(P,V)≤d(P,Q)+d(Q,V)$ (triangle inequality)\n",
    "\n",
    "In the above, $P=Q$ means P(A)=Q(A) for A⊂E , where E is the common sample space of P and Q .\n",
    "\n",
    "### Kullback-Leibler (KL) Divergence\n",
    "\n",
    "Also known as relative entropy. Let $\\mathbf{P}$ and $\\mathbf{Q}$ be discrete probability distributions with pmfs $p$ and $q$ respectively. Let's also assume P and Q have a common sample space E. Then the KL divergence (also known as relative entropy ) between $P$ and $\\mathbf{Q}$ is defined by\n",
    "\n",
    "$$\n",
    "\\text {KL}(\\mathbf{P}, \\mathbf{Q}) = \\sum _{x \\in E} p(x) \\ln \\left( \\frac{p(x)}{q(x)} \\right),\n",
    "$$\n",
    "\n",
    "\n",
    "where the sum is only over the support of $\\mathbf{P}$.\n",
    "\n",
    "$$\n",
    "\\text {KL}(\\mathbf{P}, \\mathbf{Q}) = {\\color{blue}{\\int }} _{x \\in E} p(x) \\ln \\left( \\frac{p(x)}{q(x)} \\right) dx\n",
    "$$\n",
    "\n",
    "\n",
    "Then, _I_ know\n",
    "\n",
    "### Maximum Likelihood\n",
    "\n",
    "Minimizing KL divergence means maximizing the likelihood\n",
    "$$\n",
    "L _n(x_1, \\ldots , x_ n, \\theta ) = \\prod _{i = 1}^ n p_\\theta (x_ i)\n",
    "$$\n",
    "i.e. maximizing the probability of observing sample $x_1,...,x_n$ given a certain $\\theta$. It is crucial that we interpret the likelihood $L_n$ as a function of $θ$. That is, $L_n$ varies as $θ$ ranges over the parameter space $Θ$.\n",
    "\n",
    "Likelihood for a **Bernoulli**:\n",
    "\n",
    "$$L _n(x_1, \\ldots , x_ n, p) = p^{\\sum _{i = 1}^n x_i} (1 - p)^{n - \\sum_ {i = 1}^n x_i}$$\n",
    "\n",
    "Likelihood for a **Gaussian**:\n",
    "\n",
    "$$L _n(x_1, \\ldots , x_ n, (\\mu , \\sigma ^2)) = \\prod _{i =1}^ n \\frac{1}{\\sqrt{2 \\pi } \\sigma } \\exp \\left(-\\frac{1}{2 \\sigma ^2} (x_ i - \\mu )^2\\right) =$$\n",
    "\n",
    "$$\\frac{1}{(\\sigma \\sqrt{2 \\pi })^ n} \\exp \\left(-\\frac{1}{2 \\sigma ^2} \\sum _{i = 1}^ n (x_ i - \\mu )^2\\right).$$\n",
    "\n",
    "Likelihood for a **Poisson**:\n",
    "\n",
    "$$\n",
    "L _n(x_1, \\ldots , x_ n, \\lambda ) = \\prod _{i = 1}^ n e^{-\\lambda } \\frac{\\lambda ^{x_ i}}{{x _i}!} = e^{-n \\lambda } \\frac{\\lambda ^{\\sum_ {i = 1}^ n x _i}}{x_1 ! \\cdots x_ n !}.\n",
    "$$\n",
    "Likelihood for a **Exponential**:\n",
    "\n",
    "$$L _n(x_1, \\ldots , x_ n, \\lambda ) = \\lambda^n e^{-\\lambda \\sum _{i = 1}^ n x_ i} $$\n",
    "\n",
    "Likelihood for a **Uniform**:\n",
    "\n",
    "Note that $\\displaystyle \\max _{x} f(x)$ _is_ the maximum value of the function, which is different from $\\text {argmax}f(x)$, the value of the argument $x$ at which the function is maximum.\n",
    "\n",
    "A function $g:I→R$ is **concave** (or concave down), where $I$ is an interval, if for all pairs of real numbers $x_1<x_2∈I$\n",
    "\n",
    "$$\\displaystyle \\displaystyle g(tx_1+(1-t)x_2)\\geq tg(x_1)+(1-t)g(x_2)\\qquad \\text {for all } \\, 0 < t < 1.$$\n",
    "\n",
    "A function $g:I→R$ is **convex** (or concave down), where $I$ is an interval, if for all pairs of real numbers $x_1<x_2∈I$\n",
    "\n",
    "$$\\displaystyle \\displaystyle g(tx_1+(1-t)x_2)\\leq tg(x_1)+(1-t)g(x_2)\\qquad \\text {for all } \\, 0 < t < 1.$$\n",
    "\n",
    "- concave if and only if $g′′(x)≤0$ for all $x∈I$;\n",
    "- convex if and only if $g′′(x) \\geq 0$ for all $x∈I$;\n",
    "\n",
    "$$\\displaystyle \\displaystyle \\sqrt{n} \\left(\\mathbf{g}(\\mathbf{T} _n) - \\mathbf{g}(\\vec{\\theta }) \\right) \\xrightarrow [n\\to \\infty ]{(d)} \\nabla \\mathbf{g}(\\vec{\\theta })^ T\\mathbf{T}\\, \\sim \\, \\displaystyle \\mathcal{N}\\left(0, \\nabla \\mathbf{g}(\\vec{\\theta })^ T \\Sigma_ {\\mathbf{X}} \\nabla \\mathbf{g}(\\vec{\\theta })\\right)\\qquad (\\mathbf{T}\\sim \\mathcal{N}(\\mathbf{0},\\Sigma _\\mathbf{X})).$$\n",
    "\n",
    "**Reciprocal Rule for derivative**\n",
    "\n",
    "**Matrix Determinant**\n",
    "\n",
    "**Random Variables Independence**\n",
    "\n",
    "$${\\displaystyle f_{X,Y}(x,y)=f_{X}(x)\\cdot f_{Y}(y)}$$\n",
    "\n",
    "### Fisher Information\n",
    "\n",
    "Let $θ∈Θ⊂R_d$ and let $(E,{P_θ})$ be a statistical model. Let $f_θ(x)$ be the pdf of the distribution $P_θ$. Then, the Fisher information of the statistical model is:\n",
    "\n",
    "$$I(θ)=Cov(∇ℓ(θ))=−E[Hℓ(θ)]$$\n",
    "\n",
    "where $ℓ(θ)=lnfθ(X)$. The definition when the distribution has a pmf $p_θ(x)$ is also the same, with the expectation taken with respect to the pmf. It is also defined as minus the expectation of the second derivative of the log-likelihood.\n",
    "\n",
    "$$\\displaystyle \\mathcal{I}(\\theta) = -\\mathbb E[\\ell ^{\\prime \\prime }(\\theta)]$$\n",
    "\n",
    "### Asymptotic Normality of the MLE\n",
    "\n",
    "Only applicable if:\n",
    "\n",
    "1. The parameter is identifiable.\n",
    "2. For all $\\theta \\rightarrow \\Theta$, the support of $P_\\Theta$ does not depend on $\\theta$;\n",
    "3. $\\theta^\\ast$ is not on the boundary of $\\theta$;\n",
    "4. $I(\\theta)$ is invertible\n",
    "\n",
    "Consider the statistical model $({Ber(θ)}∈(0,1))$. Let $ℓ(θ)$ denote the log-likelihood of one observation of this model. You observe samples $X_1,...,X_n∼Ber(θ∗)$ and construct the MLE $\\widehat{\\theta }_ n^{\\text {MLE}}$ for $θ^∗$. By the theorem for the convergence of the MLE (you are allowed to assume that all necessary conditions for this theorem hold), this implies that:\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\theta }_ n^{\\text {MLE}} - \\theta ^*) \\xrightarrow [n \\to \\infty ]{(d)} \\mathcal{N}(0, \\sigma ^2)$$\n",
    "\n",
    "for some constant $σ^2$ that depends on $θ^∗$.\n",
    "\n",
    "Let* $X_1,...,X_n∼iid P_{θ^∗}$ for some true parameter $θ^∗∈R_d$. We construct the associated statistical model and the maximum likelihood estimator $\\hatθ_{n}^{MLE}$ for $θ^∗$. Recall that, under some technical conditions,\n",
    "\n",
    "$$\\sqrt{n}(\\widehat{\\theta }_ n^{MLE} - \\theta) \\xrightarrow [n \\to \\infty ]{(d)} \\mathcal{N}(0, \\mathcal{I}(\\theta)^{-1})$$\n",
    "\n",
    "where $I(θ^∗)$ denotes the Fisher information. That is, the MLE $\\hatθ_{n}^{MLE}$ is asymptotically normal with asymptotic covariance matrix $\\mathcal{I}(θ^∗)^{−1}$. Standardizing the statement of asymptotic normality above we get:\n",
    "\n",
    "$$\\sqrt{n} \\mathcal{I}(\\theta)^{{\\color{blue}{a}} } (\\widehat{\\theta } _n^{MLE} - \\theta)\\xrightarrow [n \\to \\infty ]{(d)} \\mathcal{N}(0, I_{d \\times d})$$\n",
    "\n",
    "### Method of Moments\n",
    "\n",
    "$$\\widehat{\\theta } _n^{\\text {MM}} := \\psi ^{-1}\\left( \\frac{1}{n} \\sum_ {i = 1}^ n X _i, \\frac{1}{n} \\sum_ {i = 1}^ n X _i^2, \\ldots , \\frac{1}{n} \\sum_ {i = 1}^ n X_i^ d \\right)$$\n",
    "\n",
    "### M-Estimation\n",
    "\n",
    "Let $X_1,...,X_n$ be i.i.d. with some unknown distribution $P$ and an associated parameter $μ^∗$. We make no modeling assumption that $P$ is from any particular family of distributions.\n",
    "\n",
    "An M-estimator $\\hat{μ}$ of the parameter $μ^∗$ is the **argmin of an estimator of a function $Q(μ)$** of the parameter which satisfies the following:\n",
    "\n",
    "- $Q(μ)=E[ρ(X,μ)]$ for some function ρ:E×M→R, where M is the set of all possible values of the unknown true parameter $μ^∗$;\n",
    "\n",
    "- $Q(μ)$ attains a unique minimum at $μ=μ^∗$, in M. That is, $\\, \\displaystyle \\text {argmin}_{\\mu \\in \\mathcal{M}}\\mathcal{Q}(\\mu ) \\, =\\, \\mu ^*$.\n",
    "\n",
    "In* general, the goal is to find the loss function ρ such $Q(μ)=E[ρ(X,μ)]$ has the properties stated above.\n",
    "\n",
    "Note that the function $ρ(X,μ)$ is in particular a function of the random variable $X$, and the expectation in $E[ρ(X,μ)]$ is to be taken against the true distribution $P$ of $X$, with associated parameter value $μ^∗$.\n",
    "\n",
    "Because $Q(μ)$ is an expectation, we can construct a (consistent) estimator of $Q(μ)$ by replacing the expectation in its definition by the sample mean.\n",
    "\n",
    "For this example, you gain nothing. But notice that this is a generalization of MLE, so you could use just use KL and get MLE, if you use the squared difference, you get the sample mean. But you can use other distances (divergences) and get more interesting estimators.\n",
    "\n",
    "#### Cauchy Distribution\n",
    "\n",
    "$$f_ m(x) = \\frac{1}{\\pi } \\frac{1}{1 + (x - m)^2}.$$\n",
    "\n",
    "where $m$ is location parameter; $m$ is also the median, but the mean is not identifiable.\n",
    "\n",
    "#### Huber's Loss\n",
    "\n",
    "$$h_\\delta (x) = \\begin{cases} \\frac{x^2}{2} \\quad \\text {if} \\, \\, \\left| x \\right| \\le \\delta \\ \\delta ( \\left| x \\right| - \\delta /2 ) \\quad \\text {if} \\, \\, \\left| x \\right| > \\delta \\end{cases}$$\n",
    "\n",
    "#### Parametric Hypothesis Testing, Finite Sample Sizes, and Chi-Squared and Student's T Distributions\n",
    "\n",
    "Test hypotheses when the i.i.d. data samples have a Gaussian distribution.\n",
    "\n",
    "Recognize when you cannot assume the test statistic to be Gaussian (in the small sample sizes regime).\n",
    "\n",
    "#### Chi-Squared Distribution\n",
    "\n",
    "The $χ^2_d$ distribution with $d$ degrees of freedom is given by the distribution of:\n",
    "\n",
    "$$\n",
    "Z_1^2 + Z_2^2 + \\cdots + Z_ d^2,\n",
    "$$\n",
    "where $Z_1, \\ldots , Z_ d \\stackrel{iid}{\\sim } \\mathcal{N}(0,1)$; $E[X]= d$; $VAR[X]= 2d$\n",
    "\n",
    "Let $Z∼N(0,I_{d×d})$ denote a random vector whose components are standard Gaussians: $Z(1),...,Z(d)∼N(0,1)$. Which one of the following random variables has a chi-squared distribution with d degrees of freedom?\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\left|{\\boldsymbol {x}}\\right|_{2}:={\\sqrt {x_{1}^{2}+\\cdots +x_{n}^{2}}}.}\n",
    "$$\n",
    "Example: $X_1, \\ldots , X_ n \\stackrel{iid}{\\sim } \\mathcal{N}(0, \\sigma ^2)$ and let:\n",
    "\n",
    "$$\n",
    "V _n = \\frac{1}{n} \\sum_ {i = 1}^ n X_ i^2\n",
    "$$\n",
    "define $A$ so that $AV_{n}∼χ2$.\n",
    "\n",
    "$$\\frac{n}{\\sigma ^2} V_n = \\sum_ {i = 1}^ n \\frac{X _i^2}{\\sigma ^2}= \\sum_ {i = 1}^ n \\left(\\frac{X_ i}{\\sigma }\\right)^2,$$\n",
    "\n",
    "#### Cochran's Theorem\n",
    "\n",
    "Cochran's theorem states that if $X_1,...,X_n \\stackrel{iid}{\\sim } \\mathcal{N} (μ,σ^2)$, then the sample variance\n",
    "\n",
    "$$S _n := \\frac{1}{n} \\left(\\sum_ {i = 1}^ n X _i^2\\right) - (\\overline{X}_ n)^2$$\n",
    "\n",
    "satisfies:\n",
    "\n",
    "- $\\overline{X}_ n$ is independent of $S_n$, and\n",
    "- $\\frac{nS_n}{σ^2}∼χ^{2}_{n−1}$ .\n",
    "\n",
    "Relates the sample variance and sample mean when the data samples are i.i.d. Gaussian.\n",
    "\n",
    "#### Student's T Distribution\n",
    "\n",
    "The definition of the student's T distribution with $n−1$ degrees of freedom is that it is given by the distribution of $\\frac{Z}{\\sqrt{V/(n−1)}}$ where $Z∼N(0,1)$, $V∼χ^2_{n−1}$ and $Z$ and $V$ are independent (by Cochrane). Since we are dividing by $V$, a $χ^2$ random variable, then $T_n$ will not have the same distribution as $N(0,1)$ for all $n≥2$.\n",
    "\n",
    "Now consider the test statistics:\n",
    "\n",
    "$$T_{n} := \\sqrt{n} \\left( \\frac{\\overline{X}_ n - \\mu }{\\sqrt{\\frac{1}{n- 1} \\sum _{i = 1}^ n (X_ i - \\overline{X}_ n)^2} } \\right).$$\n",
    "\n",
    "Student's T test for non-asymptotic level $α$ is **only applicable** if $X$ is a Gaussian random variable; in that case only the test statistic follow a Student's T distribution for a finite number of samples $n$.\n",
    "\n",
    "The denominator represents the unbiased estimator (see the $n-1$) for the sample standard deviation. It $T_n$ follows a student t distribution (it is a key assumption that the data is Gaussian. Otherwise, the test statistic $T_n$ will not necessarily follow the student's T distribution and, hence, may not even be pivotal). The Student's t distribution converges to the standard normal with increasing sample sizes.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Non Asymptotic\n",
    "- Can be run on small samples\n",
    "- Can also use it for large sample Sizes\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "- It assumes Gaussian samples\n",
    "\n",
    "#### Wald's test\n",
    "\n",
    "Starting from the fact that (exploiting the normality of the MLE):\n",
    "\n",
    "$$\\sqrt{n} \\mathcal{I}(\\theta)^{{{\\frac{1}{2}}} } (\\widehat{\\theta } _n^{MLE} - \\theta)\\xrightarrow [n \\to \\infty ]{(d)} \\mathcal{N}(0, I_{d \\times d})$$\n",
    "\n",
    "The Wald test, by taking a quadratic form, reduces the test problem to a one-sided one-dimensional comparison. The test is of the form:\n",
    "\n",
    "$$\\left| \\sqrt{n}\\, \\mathcal{I}(\\mathbf{0})^{1/2}(\\widehat{\\theta } _n^{MLE}- \\mathbf{0}) \\right| ^2 \\xrightarrow [n\\to \\infty ]{(d)} \\chi ^2_ k\\,$$\n",
    "\n",
    "where $k$ is the number of parameters. This is done **under the NULL hypothesis**.\n",
    "\n",
    "_Remark_: Real matrices satisfying $\\, \\mathbf{M}^{T}=\\mathbf{M}^{-1}\\,$ (or equivalently $\\, \\mathbf{M}\\mathbf{M}^ T=\\mathbf{M}^ T\\mathbf{M}=\\mathbf{1}_{d\\times d},\\,$) are called orthogonal matrices. In general, in $d$ dimensions and for any orthogonal matrix $\\, \\mathbf{M},\\,$, $\\, \\mathbf{MZ},\\,$ is also a standard multivariate Gaussian vector if $\\mathbf{Z}$ is a standard multivariate Gaussian.\n",
    "\n",
    "#### Likelihood ratio test\n",
    "\n",
    "$$\\psi _C = \\mathbf{1}\\left( \\frac{L_ n(x_1, \\ldots , x_ n; \\theta _1 )}{L_ n(x_1, \\ldots , x_ n; \\theta _0 )} > C \\right).$$\n",
    "\n",
    "Perform the likelihood ratio test for a family of hypothesis testing questions. Use an asymptotically normal estimator to test implicit hypotheses involving an unknown parameter, i.e.:\n",
    "\n",
    "$$\\psi _C = \\mathbf{1}\\left( \\frac{L_ n(x_1, \\ldots , x_ n; \\widehat{\\theta _n}^{MLE})}{L_ n(x_1, \\ldots , x_ n; \\theta _0 )} > C \\right).$$\n",
    "\n",
    "which becomes (taking log and multiplying by 2):\n",
    "\n",
    "$$T _n = 2 \\left( \\ell_ n(\\widehat{\\theta _n}^{MLE}) - \\ell_ n(\\widehat{\\theta _ n}^{c}) \\right)$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$T _n \\xrightarrow [n \\to \\infty ]{(d)} \\chi_ {d-r}^2$$\n",
    "\n",
    "#### Multinomial Distribution\n",
    "\n",
    "[here (part 4)](https://courses.edx.org/courses/course-v1:MITx+18.6501x+3T2019/courseware/unit4/u04s05_hypotesting/1?activate_block_id=block-v1%3AMITx%2B18.6501x%2B3T2019%2Btype%40vertical%2Bblock%40u04s05_hypotesting-tab1) - copy it\n",
    "\n",
    "#### Goodness of fit\n",
    "\n",
    "Suppose you observe iid samples $X_1, \\ldots , X_ n \\sim P$ from some **unknown** distribution $\\mathbf{P}$. Let $F$ denote a family of known distributions (e.g, F it be the family of normal distributions ${ \\mathcal{N}(\\mu , \\sigma ^2) }_{\\mu \\in \\mathbb {R}, \\sigma ^2 > 0}$). In the topic of goodness of fit testing, our goal is to answer the question \"Does $\\mathbf{P}$ belong to the family $F$?\".\n",
    "\n",
    "#### Chi Square Test\n",
    "\n",
    "**for Multinomial distributions** Let $\\widehat{\\mathbf{p}}$ denote the MLE for a categorical statistical model $( { a_1, \\ldots , a_ K } , { \\mathbf{P}_{\\mathbf{p}} }_ {\\mathbf{p} \\in \\Delta _ K})$ . Let p∗ denote the true parameter. Then n−−√(pˆ−p∗) is asymptotically normal and\n",
    "\n",
    "$$n \\sum_{i = 1}^K \\frac{(\\widehat{p_ i} - p_i^\\ast)^2}{p_i^\\ast} \\xrightarrow [n \\to \\infty ]{(d)} \\chi_{K -1}^2$$\n",
    "\n",
    "This test derives from the Wald's test, and $K-1$ degrees of freedom derive from the dependency between the parameters in the multinomial.\n",
    "\n",
    "<span style=\"color:blue\">The test is asymptotic (e.g. only applies for $n \\rightarrow \\infty$)</span>\n",
    "\n",
    "y\n",
    "\n",
    "#### Empirical CDF\n",
    "\n",
    "The empirical cumulative distribution function , also called the empirical cdf, is the random function:\n",
    "\n",
    "$$\\displaystyle t \\mapsto \\frac{1}{n} \\sum _{i = 1}^ n \\mathbf{1}(X_ i \\leq t).$$\n",
    "\n",
    "_Law of Large Numbers_ $\\mapsto$ uniformly $\\mapsto$ **Glivenko Cantelli**\n",
    "\n",
    "_Central Limit Theorem_ $\\mapsto$ uniformly $\\mapsto$ **Donsker's Theorem**\n",
    "\n",
    "**Donsker's Theorem** states that, looking at the worse possible $t$:\n",
    "\n",
    "$$\\sqrt{n} \\sup _{t \\in \\mathbb {R}} |F_ n(t) - F(t)| \\xrightarrow [n \\to \\infty ]{(d)} \\sup_{0 \\leq x \\leq 1} |\\mathbb {B}(x)|,$$\n",
    "\n",
    "where $\\mathbb {B}(x)$ is a [brownian bridge](https://en.wikipedia.org/wiki/Brownian_bridge).\n",
    "\n",
    "#### Kolmogrov-Smirnoff Test\n",
    "\n",
    "The Kolmogorov-Smirnov test statistic is defined as (also called $L_\\infty$ distance)\n",
    "\n",
    "$${T _n = \\sup_{t \\in \\mathbb {R}} \\sqrt{n} \\bigg| F_n(t) - F^0(t) \\bigg|}$$\n",
    "\n",
    "and the Kolmogorov-Smirnov test is\n",
    "\n",
    "$$\\displaystyle \\displaystyle \\mathbf{1}(T _n>q_\\alpha )\\qquad \\text {where } q_\\alpha =q_\\alpha (\\sup_{t \\in [0,1]}\\left| \\mathbb {B}(t) \\right|).$$\n",
    "\n",
    "By definition, $T_n$ is a pivotal statistic under $H_0$.\n",
    "\n",
    "[**Pivotal distribution** is a distribution which does not depend on unknown parameters, so you can compute quantiles.]\n",
    "\n",
    "Here, $q_\\alpha =q_\\alpha (\\sup_{t \\in [0,1]}\\left| \\mathbb {B}(t) \\right|)\\,$ is the $(1−α)$ quantile of the supremum of the Brownian bridge as in Donsker's Theorem. It can be computed explicitly as follows:\n",
    "\n",
    "$$\\displaystyle Tn = \\sqrt{n}\\sup _{t \\in \\mathbb {R}} \\bigg| F_ n(t) - F^0(t) \\bigg|$$\n",
    "\n",
    "$\\sqrt{n}\\max_{i=1,\\ldots ,n}\\left{ \\max \\left(\\left| \\frac{i-1}{n}-F^0(X_{(i)}) \\right|,\\left| \\frac{i}{n}-F^0(X_{(i)}) \\right| \\right) \\right}$\n",
    "\n",
    "[INVERSE TRANSFORM SAMPLING](https://en.wikipedia.org/wiki/Inverse_transform_sampling)\n",
    "\n",
    "where X(i) is the order statistic , and represents the i(th) smallest value of the sample. For example, X(1) is the smallest and X(n) is the greatest of a sample of size n .\n",
    "\n",
    "```python\n",
    "def T(n):\n",
    "    SIMSIZE=1000000\n",
    "    i = np.arange(1,n+1)[:,np.newaxis]\n",
    "    u = np.random.uniform(size=(n,SIMSIZE))\n",
    "    u.sort(axis=0)\n",
    "    return np.max( np.maximum(np.abs((i-1)/n - u), np.abs(i/n - u)), axis=0 )\n",
    "```\n",
    "\n",
    "#### Other Goodness of Fit Tests\n",
    "\n",
    "- Cramer - Von Mises ($L_2$)\n",
    "- [...]\n",
    "\n",
    "#### Kolmogrov-Lilliefors Test\n",
    "\n",
    "Till now, $F$ was completely deterministic, with given parameters. But what if we want to test whether a distribution is e.g. Gaussian, no matter the parameters?\n",
    "\n",
    "First, I substitute mean and variance with sample ones. But this makes $F$ artificially more stick to the data. But if we plug in estimators for $μ$ and $σ^2$ (and not their true values), the Donker's convergence no longer holds.\n",
    "\n",
    "The **Kolmogorov-Smirnov** test was designed to test if the data has a specific distribution; it is not useful for deciding whether or not the true distribution $\\, \\mathbf{P}\\,$ lies in a given family of distributions. In this case we use the **Kolmogrov-Lilliefors** test.\n",
    "\n",
    "#### QQ plots\n",
    "\n",
    "[Here](https://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot) and [here](https://seankross.com/2016/02/29/A-Q-Q-Plot-Dissection-Kit.html)\n",
    "\n",
    "### Bayesian Statistics\n",
    "\n",
    "In the Bayesian set-up, **we do not even assume that there exists a true parameter**, or at least we model it as a random variable to represent our uncertainty. we use the data to update our prior belief about a parameter and transform it into a posterior belief, which is reflected by a posterior distribution. In this framework, **we model the true parameter as a random variable** and update its distribution as we receive more data.\n",
    "\n",
    "Usually a **beta distribution** is used to represent the distribution of the prior, especially if we are dealing with a Bernoulli random variable. Recall that the Beta distribution in $x$ is defined as the distribution with support $[0,1]$ and pdf:\n",
    "\n",
    "$$C(\\alpha , \\beta ) x^{\\alpha -1}(1-x)^{\\beta -1}$$\n",
    "\n",
    "where $α$ and $β$ are parameters that satisfy $α>0$, $β>0$. Here, $C(α,β)$ is a normalization constant that does not depend on $x$ (simulate a beta distribution [**here**](http://eurekastatistics.com/beta-distribution-pdf-grapher/)).\n",
    "\n",
    "After observing the available sample $X_1, . . . ,X_n$ we can update our belief about $p$ by taking its distribution conditionally on the data, which is called the posterior distribution.\n",
    "\n",
    "We start with the conditional likelihood:\n",
    "\n",
    "$$L(X_1, \\ldots , X_ n | \\theta )$$\n",
    "\n",
    "**Note**: this is exactly the same likelihood we used in the $MLE$ approach.\n",
    "\n",
    "We want to get:\n",
    "\n",
    "$$\\pi (\\theta |X_1, \\ldots , X_ n)$$\n",
    "\n",
    "In this way:\n",
    "\n",
    "$$\\displaystyle \\pi (\\theta |X_1,\\dots ,X_ n)=\\frac{L_n(X_1,\\dots ,X_ n|\\theta )\\pi (\\theta )}{\\int_\\Theta L_n(X_1,\\dots ,X_ n|t)\\pi (t)\\; dt}$$\n",
    "\n",
    "which is equal to say:\n",
    "\n",
    "$$\\displaystyle \\pi (\\theta |X_1,\\dots ,X_ n)\\propto L_n(X_1,\\dots ,X_ n|\\theta )\\pi (\\theta )$$\n",
    "\n",
    "Since the denominator is independent of \\theta\n",
    "\n",
    "In the bernoulli case:\n",
    "\n",
    "$$p _n(X_1,\\dots ,X_ n|\\theta )=\\theta ^{\\sum _{i=1}^ n X_ i}(1-\\theta )^{n-\\sum _{i=1}^ n X_ i}.$$\n",
    "\n",
    "$$\\displaystyle \\pi (\\theta |X_1,\\dots ,X_ n) \\propto p _n(X_1,\\dots ,X_ n|\\theta )\\pi (\\theta )$$\n",
    "\n",
    "$$\\displaystyle ... \\propto \\theta ^{a-1}(1-\\theta )^{b-1}\\theta ^{\\sum _{i=1}^ n X_ i}(1-\\theta )^{n-\\sum _{i=1}^ n X_ i}$$\n",
    "\n",
    "$$\\displaystyle ... \\propto \\theta ^{a+\\sum _{i=1}^ n X_ i -1}(1-\\theta )^{b+n-\\sum _{i=1}^ n X_ i -1}.$$\n",
    "\n",
    "According to Bayes' rule, the posterior distribution (up to a constant of proportionality) is computed by multiplying the prior and posterior distributions taken as a function of the parameter. As a result, we need the full distribution for $π(\\theta)$ as well as the likelihood function $L(X_1,X_2,...,X_n|\\theta)$.\n",
    "\n",
    "#### Uniformative priors\n",
    "\n",
    "When I don't have any prior knowledge:\n",
    "\n",
    "- if $\\Theta$ is bounded: **uniform**\n",
    "- if $\\Theta$ is unbounded: **improper prior**, we set formally equal to $1$, which integrates to $\\infty$\n",
    "\n",
    "For both of them, $\\pi(\\theta) = 1$\n",
    "\n",
    "#### Jeffrey's prior\n",
    "\n",
    "\"Invariant under rescaling of the parameter\". It's the _non-informative_ prior equal to:\n",
    "\n",
    "$$\\pi_{J}(\\theta ) \\propto \\sqrt{\\text {det}I(\\theta )},$$\n",
    "\n",
    "Where $det$ is the determinant (trying to fit everyone in one number) and $I$ is the Fisher Information. So more weight is given when $I(θ)$ is high. The Fisher information is also the reciprocal of the MLE variance, so when the Fisher information is high, the MLE variance is low and thus the MLE has less uncertainty. Combining, we get that the Jeffreys prior gives more weight to values of θ whose MLE estimate has less uncertainty.\n",
    "\n",
    "Continuing from the above reasoning, when the MLE estimate has less uncertainty and we are able to estimate it more precisely. This corresponds to the data giving more information about the parameter when the Jeffreys prior yields larger values.\n",
    "\n",
    "Again, Jeffreys prior gives more weight to regions with high Fisher information . By the given interpretation for the Fisher information, this means that at these areas, a small change to θ will influence the data relatively more, or in other words, potential outcomes are more sensitive to slight changes in θ .\n",
    "\n",
    "#### Conjugate Priors\n",
    "\n",
    "One side concept introduced in the second Bayesian lecture is the conjugate prior. Simply put, a prior distribution π(θ) is called conjugate to the data model, given by the likelihood function L(Xi|θ) , if the posterior distribution π(θ|X1,X2,...,Xn) is part of the same distribution family as the prior.\n",
    "\n",
    "> The way I understand this is seeing the Fisher information (and its square root, that is, Jeffrey's prior) as a function of (\\theta), i.e. the parameter we are estimating. This parameter lives in a certain space, and some parts of that space may be \"flat\", in the sense that if you choose a point or another in that region the model will \"fit\" the data similarly. On the other hand, there are regions for which small changes to (\\theta) make big differences to how well the model fits the data. These later regions have high Fisher information.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "Given a joint probability distribution P for the random pair $(X,Y)$, the regression function of Y with respect to X is defined as\n",
    "\n",
    "$$\\nu (x) = \\mathbb E[Y | X = x] = \\sum_{\\Omega_Y} y \\cdot \\mathbf{P}(Y = y \\; |\\; X = x)$$\n",
    "\n",
    "which tells us the average value of $Y$ given the knowledge that $X=x$. In the case of continuous distributions where we can compute the conditional density $f(y|x)$, the expression on the right hand side is replaced with an integral:\n",
    "\n",
    "$$\\mathbb E[Y | X = x] = \\int_{\\Omega_Y} y f(y | x) dy$$\n",
    "\n",
    "In Linear Regression , we will work with the assumption that the regression function<br>\n",
    "$ν(x):=E[Y|X=x]$ is linear, so that\n",
    "\n",
    "$$ν(x)=a+bx$$\n",
    "\n",
    "we will be studying the Least Squares Estimator. It is an estimator $(\\hat{a}, \\hat{b})$ so that $\\hat{Y}=\\hat{a}+\\hat{b}X$ is \"close\" (in some distance metric) to the actual $Y$ as often as possible. Assume $Var(X)≠0$. The **theoretical linear (least squares) regression** of $Y$ on $X$ prescribes that we find a pair of real numbers $a$ and $b$ that **minimize** $E[(Y−a−bX)^2]$, over all possible choices of the pair $(a,b)$; the $a$ and $b$ that minimize the squared error are:\n",
    "\n",
    "$$a = \\mathbb E[Y] - \\frac{\\textsf{Cov}(X,Y)}{\\textsf{Var}(X)} \\mathbb E[X], \\qquad b = \\frac{\\textsf{Cov}(X,Y)}{\\textsf{Var}(X)}$$\n",
    "\n",
    "In **empirical linear regression**, we are given a collection of points ${(x_i, y_i) }_{i=1}^{n}$. The goal is to fit a linear model $Y=a+bX+ε$ by computing the Least Squares Estimator, which minimizes the loss function\n",
    "\n",
    "$$\\frac{1}{n} \\sum_{i=1}^ n (y_i - (a + bx_i))^2.$$\n",
    "\n",
    "Using the same technique as in the problems on theoretical linear regression, one obtains the solution\n",
    "\n",
    "$$\\hat{a} = \\overline{y} - \\frac{\\overline{xy} - \\overline{x}\\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2} \\overline{x} \\qquad \\hat{b} = \\frac{\\overline{xy} - \\overline{x}\\cdot \\overline{y}}{\\overline{x^2} - \\overline{x}^2}.$$\n",
    "\n",
    "In this particular case, this is precisely what one obtains by taking the least squares solution for the theoretical linear regression problem and replacing each term with their empirical counterparts according to the plug-in principle.\n",
    "\n",
    "> The **rank** of a matrix is defined as (a) the maximum number of linearly independent column vectors in the matrix or (b) the maximum number of linearly independent row vectors in the matrix. Both definitions are equivalent. For an r x c matrix, If r is less than c, then the maximum rank of the matrix is r.\n",
    "\n",
    "The model is homoscedastic if $ε_1,...,ε_n$ are i.i.d.\n",
    "\n",
    "#### Multivariate Case\n",
    "\n",
    "analytic computation of the LSE (which is also MLE) yields:\n",
    "\n",
    "$$\\hat{{\\boldsymbol \\beta }} = (\\mathbb {X}^ T \\mathbb {X})^{-1} \\mathbb {X}^ T \\mathbf Y.$$\n",
    "\n",
    "And it is distributed:\n",
    "\n",
    "$$\\hat{{\\boldsymbol \\beta }} \\sim \\mathcal{N}(\\beta , \\sigma ^2 (\\mathbb {X}^ T \\mathbb {X})^{-1}).$$\n",
    "\n",
    "$Y$ is distributed:\n",
    "\n",
    "$$Y∼N(X^{T}β,σ^2I_{n})$$\n",
    "\n",
    "And, for instance, in the one dimensional case, if we assume that $ε∼N(0,σ^2I_{1000})$ for some fixed $σ^2$, so that $Y∼N(Xβ,σ^2I_{1000})$. The quadratic risk of $\\hat{β}$ and the prediction error are respectively:\n",
    "\n",
    "$$\\mathbb E[| \\hat{{\\boldsymbol \\beta }} - {\\boldsymbol \\beta }|_2^2 = \\sigma^2 \\mathrm{tr}((\\mathbb {X}^ T\\mathbb {X})^{-1})$$\n",
    "\n",
    "$$\\mathbb E[ | \\mathbf Y- \\mathbb {X}\\hat{{\\boldsymbol \\beta }} |_2^2 ] = \\sigma^2(n-p)$$\n",
    "\n",
    "$tr$ means_ \"trace\" and it is the sum of all the diagonal entries.\n",
    "\n",
    "Doing inference regarding regression means producing **non-asymptotic** estimates\n",
    "\n",
    "<span style=\"color:blue\">RECITATION 23 FOR SPECTRAL THEOREM, ORTHOGONAL AND ORTHONORMAL VECTORS</span>\n",
    "\n",
    "; this will be the basis of PCA; given a _symmetric square matrix_ $A$, it can be decomposed in:\n",
    "\n",
    "$$A = V \\Lambda V^T$$\n",
    "\n",
    "Let $A$ be a square $n×n$ matrix with n linearly independent eigenvectors qi (where i = 1, ..., n). Then A can be factorized as\n",
    "\n",
    "$${\\displaystyle \\mathbf {A} =\\mathbf {Q} \\mathbf {\\Lambda } \\mathbf {Q} ^{-1}}$$\n",
    "\n",
    "where $Q$ is the square $n×n$ matrix whose $i^{th}$ column is the eigenvector $q_i$ of $A$, and $Λ$ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, $Λ_{ii} = λ_i$.\n",
    "\n",
    "where $\\Lambda$ is a diagonal matrix and $V$ is a collection of orthogonal vectors\n",
    "\n",
    "**Projection matrixes**, then eigenvalues are $+1$ or $-1$\n",
    "\n",
    "Consider the model Y|X∼N(XTβ,1) , where X is a p -dimensional random variable. Here, β is a fixed constant. Indicate whether the following statements are true, or false.\n",
    "\n",
    "- $E[Y|X]$ is a constant random variable.\n",
    "- The expected value of $Y$, $E[Y]$ is a constant random variable, if we assume that each Xi has mean μ. Indeed, $\\mathbb {E}[Y]=\\mathbb {E}[\\mathbb {E}[Y|\\mathbf X]]=\\mathbb {E}[\\mathbf X^ T\\beta ]=\\sum_{i=1}^ p \\beta_i\\mu$, which is constant, using the law of iterated expectations.\n",
    "- If $X_i$'s are iid Gaussian, then the conditional mean, $E[Y|X]$ is a Gaussian random variable. Indeed, $\\mathbf X^ T\\beta = \\sum_{i=1}^ p X_ i\\beta_i$ is a sum of iid Gaussian random variables, and is itself a Gaussian random variable.\n",
    "\n",
    "### Generalized Linear models\n",
    "\n",
    "$Y | X=x \\mathcal{}$ some family distribution, these family is the family for which the conditions of the asymptotic normality of the central limit theorems uphold, and the other ones won't be part of this family (bernoulli, exponential, Poisson).\n",
    "\n",
    "As it turns out, this comes at a cost: finding the Maximum Likelihood Estimator becomes more difficult (in general). We relax the assumption that $μ$ is linear. Instead, we assume that $g∘μ$ is linear, for some function $g$:\n",
    "\n",
    "$$g(\\mu (\\mathbf x)) = \\mathbf x^T \\beta.$$\n",
    "\n",
    "The function $g$ is assumed to be known, and is referred to as the **link function**. Through an appropriate choice of the link function, which depends on the model, we should be able to compute an estimator $\\hat{β}$, usually the MLE.\n",
    "\n",
    "#### Exponential Families\n",
    "\n",
    "Recall that the one-parameter canonical exponential family have pdf/pmf parametrized by $θ$ of the form\n",
    "\n",
    "$$\\displaystyle \\displaystyle f_\\theta (y) = \\exp \\left( \\frac{y \\theta - b(\\theta )}{\\phi } + c(y,\\phi ) \\right)$$\n",
    "\n",
    "where $b$ and $c$ are known functions, and $ϕ$ is a known number referred to as the dispersion parameter. The function $b(θ)$ is also known as the log-partition function.\n",
    "\n",
    "Note that b(θ) does not depend on y and c(y,ϕ) does not depend on θ .\n",
    "\n",
    "**Fisher info refresh and derivation** <https://courses.edx.org/courses/course-v1:MITx+18.6501x+3T2019/courseware/unit3/u03s04_methodestimation/3>\n",
    "\n",
    "**Link function** goes from $\\mu$ to $X^T\\beta$, not the other way around\n",
    "\n",
    "The canonical exponential families, parametrized by $θ$, with the log-partition function $b(θ)$ having the property that $b′(θ)=μ$. Recall that in GLMs, the point of the link function is to assume $g(μ(x))=x^Tβ$, where $μ$ is the regression function: the mean of $Y$ given $X=x$, $E[Y|X=x]$.\n",
    "\n",
    "Based on the properties of the log-partition function $b$, we derived previously that $b′(θ)=μ$, so we have the identity $g(μ)=(b′)^{−1}(μ)$.\n",
    "\n",
    "The assumptions of a distribution for Y and a link function g(μ(x)) relate Y and X=x through the following equation:\n",
    "\n",
    "$$\\displaystyle g(\\mu (\\mathbf{x})=\\mathbb {E}[Y | \\mathbf{X}=\\mathbf{x}]) = \\mathbf{x}^ T {\\boldsymbol \\beta }.$$\n",
    "\n",
    "**Poisson Case**: the function $b(θ)=e^θ$ for the Poisson exponential family. Further, $ϕ=1$ for the Poisson exponential family. Therefore, the log-likelihood function\n",
    "\n",
    "$$\\displaystyle \\ell _n(\\mathbf Y,\\mathbb {X},{\\boldsymbol \\beta }) = \\sum_ i \\frac{Y _i h(X_ i^ T {\\boldsymbol \\beta }) - b(h(X_ i^ T {\\boldsymbol \\beta }))}{\\phi } + c$$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$\\displaystyle \\ell _n(\\mathbf Y,\\mathbb {X},{\\boldsymbol \\beta }) = \\sum_ i \\left(Y _i X_ i^ T {\\boldsymbol \\beta }- e^{X_ i^ T {\\boldsymbol \\beta }}\\right) + c.$$\n",
    "\n",
    "[**Chain Rule Reminder**: $(f\\circ g)'=(f'\\circ g)\\cdot g'.$]\n",
    "\n",
    "### PCA\n",
    "\n",
    "Given the vector of dimension $1 \\times d$\n",
    "\n",
    "$$\\mathbf X_i = (X_i^{(1)}, ..., X_i^{(d)})^T, \\space\\space\\space i = 1,..., n$$\n",
    "\n",
    "Let $X_i,i=1,...,n$ be iid data points in $\\mathbb {R}^d$. As presented in the lecture and given in the slides, the empirical covariance matrix is:\n",
    "\n",
    "$$\\displaystyle S \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\left(\\mathbf X_i \\mathbf X_i^ T \\right) - \\overline{\\mathbf X}~ \\overline{\\mathbf X}^T,$$\n",
    "\n",
    "where empirical mean $\\overline{\\mathbf X}$ is:\n",
    "\n",
    "$$\\displaystyle \\overline{\\mathbf X} \\triangleq \\frac{1}{n} \\sum_{i=1}^{n} \\mathbf X_i = (\\overline{\\mathbf X}^{(1)}, ..., \\overline{\\mathbf X}^{(d)})^T = \\frac{1}{n} \\mathbb X^T \\mathbb{1}$$\n",
    "\n",
    "#### TODO\n",
    "\n",
    "**Write Fisher info as second derivative, draw beta distribution in Bernoulli case and show with simulations how for close to 0 and 1 the asymptotic variance is lower; compute inverse of a function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential Distribution\n",
    "\n",
    "In probability theory and statistics, the **exponential distribution** (also known as the negative exponential distribution) is the probability distribution that describes the time between events in a Poisson point process, i.e., **a process in which events occur continuously and independently at a constant average rate**. An example from finance can be the\n",
    "\n",
    "$ f(x;\\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} & x \\ge 0, \\ 0 & x < 0\\. \\end{cases}$\n",
    "\n",
    "The expected value of the exponential distribution is:\n",
    "\n",
    "${\\displaystyle \\operatorname {E} [X]={\\frac {1}{\\lambda }}}$\n",
    "\n",
    "hence, given the expected value, $\\lambda$, can be obtained.\n",
    "\n",
    "![](C:\\Users\\luca\\Google Drive\\NoViews\\Graphs\\exponential_distribution.png)\n",
    "\n",
    "Memorylessness:\n",
    "\n",
    "[log change vs percentage change](https://stats.stackexchange.com/questions/244199/why-is-it-that-natural-log-changes-are-percentage-changes-what-is-about-logs-th)\n",
    "\n",
    "[**1\\. Probability Notes**](#ProbabilityNotes) [**2\\. Tail Risk**](#TailRisk) [**3\\. Kelly Principle**](#Kelly) [**4\\. Questions**](#Questions)\n",
    "\n",
    "#### Martingale\n",
    "\n",
    "In probability theory, a martingale is a sequence of random variables (i.e., a stochastic process) for which, at a particular time, **the conditional expectation of the next value in the sequence, given all prior values, is equal to the present value**.\n",
    "\n",
    "#### Brier Score\n",
    "\n",
    "The **Brier score** is a proper score function that measures the accuracy of probabilistic predictions\n",
    "\n",
    "The Brier score can be thought of as either a measure of the \"calibration\" of a set of probabilistic predictions, or as a \"cost function\". More precisely, across all items ${i\\in {1...N}} $ in a set N predictions, the Brier score measures the mean squared difference between:\n",
    "\n",
    "- The predicted probability assigned to the possible outcomes for item i\n",
    "- The actual outcome {\\displaystyle o_{i}} o_i\n",
    "\n",
    "$BS = \\frac{1}{N}\\sum\\limits _{t=1}^{N}(f_t-o_t)^2 \\,!$\n",
    "\n",
    "Più precisamente: se le code delle distribuzioni elementari tendono a zero più lentamente di |X|-(N+1), i momenti di ordine superiore a N non esisteranno. Distribuzioni stabili Nei primi decenni del XX secolo ci si chiese se esistesse una versione più generale del CLT applicabile a situazioni in cui le variabili componenti hanno varianza infinita. Una risposta fu trovata nel 1955 da Gnedenko e Kolmogorov, il cui Teorema Generalizzato del Limite Centrale (GCLT) dice che se N è compreso tra 0 e 2, la distribuzione della somma di variabili aleatorie equidistribuite con code che tendono a zero come |X|-(N+1) tende alla forma di una \"Distribuzione (alfa-)stabile di Lévy\" (simmetrica). Questa famiglia di distribuzioni era stata studiata da Paul Lévy gia' negli anni '20, ed è caratterizzata da quattro parametri: - Il primo, α, è compreso tra 0 e 2 e determina la rapidità di caduta delle code; in particolare, se α < 2 le code tendono a zero come |X|-(α+1), e la distribuzione è detta \"stabile paretiana\"; ma se α = 2 la caduta bruscamente diviene molto più rapida, dell'ordine di exp(-x2) come nel caso gaussiano; in effetti, in questo caso la distribuzione si riduce a una gaussiana. - Il secondo, β, e' compreso tra -1 e +1 e determina l'asimmetria (skew) della PDF. Se β = 0 la PDF e' simmetrica, e se in aggiunta α = 1 la distribuzione si riduce a una Lorenziana. - Il terzo, γ, è legato alla \"larghezza\" della curva. Se α = 2, γ rappresenta la deviazione standard; se α = 1, γ rappresenta il parametro dello stesso nome nella Lorenziana, e coincide con la metà dello scarto interquartile. - Il quarto, δ, è legato alla posizione del picco (moda, che in questa famiglia di distribuzioni è sempre unica). L'aggettivo \"stabile\" nel nome dipende dalla più importante proprietà della famiglia: se due variabili aleatorie seguono distribuzioni stabili di Lévy con il medesimo α, la loro somma seguirà anch'essa una distribuzione stabile di Lévy con lo stesso valore di α. Un'altra interessante proprietà è la seguente: così come per le gaussiane la somma di due variabili indipendenti ha un γ il cui quadrato è la somma dei quadrati degli y delle due componenti (perché nella gaussiana γ rappresenta la varianza, che per variabili indipendenti è additiva), così per la somma di due variabili indipendenti stabili di Lévy vale la relazione generalizzata: γα = γ1α + γ2α Questa proprieta' formale, assieme alla relazione allo scarto interquartile relativamente simile nei casi di α = 2 e α = 1, porta a considerare γ un buon candidato per rimpiazzare la deviazione standard quando la varianza semplicemente non esiste. Nel caso di covarianze di più variabili, dovrebbe essere possibile fare lo stesso con i parametri di distribuzioni stabili multivariate. Infine, una breve nota su un caso diverso ma correlato. Talora, per esempio nel campo assicurativo, non interessano variabili che siano la somma (o la combinazione lineare a coefficienti costanti) di più variabili elementari, ma altre che siano il massimo (o il minimo) di tali variabili elementari. Per questo, sono state studiate distribuzioni che godono della proprietà di stabilità rispetto alle operazioni di massimo e di minimo, anziché di somma: sono quelle appartenenti alla famiglia di \"distribuzioni di valore estremo generalizzate\" (GEV), sviluppate nell'ambito della teoria del valore estremo (EVT).\n",
    "\n",
    "Predictions have two types of uncertainty; aleatory and epistemic.\n",
    "\n",
    "#### PROBABILITY CALIBRATION\n",
    "\n",
    "To be calibrated, events that it predicts to be 90% likely should occur 90% of the time. Events that it predicts to be 10% likely should occur 10% of the time.\n",
    "\n",
    "_Calbiration formula_\n",
    "\n",
    "A desirable property of a prediction market is that it is calibrated.\n",
    "\n",
    "#### Shannon's Demon\n",
    "\n",
    "We can simplify the idea of volatility harvesting through a thought-experiment developed in the 1940s by Claude Shannon. The experiment, known as Shannon's Demon, showed how it was possible to profit even from markets that were characteristic of a random walk, as long as they were volatile. Imagine a situation in of a risky asset, that can double or halve at each step with equal probability $0.5$. You can decide to hold a fixed proportion $f$ of cash at each step. The graph shows a simulation of $1000$ steps, with different proportion of $f$, from $10\\%$ up to $100\\%$, i.e. the asset itself. Below the realization of the simulation (for sake of comparison, the same draws are assumed)\n",
    "\n",
    "![](C:\\Users\\luca\\Google Drive\\NoViews\\Graphs\\demo2.png)\n",
    "\n",
    "Counterintuitively, while the strategy of holding the asset only (no cash) let to a performance of basically $-100\\%$, the strategy of rebalancing with $50\\%$ cash at each step yield an astonishing return of roughly $10^{20}$. Mind Blowing. How is this possible?\n",
    "\n",
    "It can help to think about another example. We simulated 100 draws from the previous situation, then we applied a daily rebalancing with a fixed percentage of cash (on the x axis). We repeated this for 100 times (different asset realizations). Below the results:\n",
    "\n",
    "![](C:\\Users\\luca\\Google Drive\\NoViews\\Graphs\\demo3.png)\n",
    "\n",
    "![](C:\\Users\\luca\\Google Drive\\NoViews\\Graphs\\demo5.png)\n",
    "\n",
    "Performance of passive and active investment styles\n",
    "\n",
    "```\n",
    "    | Heads | Tails | gROWTH\n",
    "```\n",
    "\n",
    "------- | ----- | ----- | --------------------------------- Asset 1 | +40% | -30% | $½ ln(1.4) + ½ ln(0.7) ≈ -0.010$ Asset 2 | -20% | +15% | $½ ln(0.8) + ½ ln(1.15) ≈ -0.042$\n",
    "\n",
    "Heads Tails Heads Tails Net return on assets +40% -30% -20% +15% Growth rate of assets ½ ln(1.4) + ½ ln(0.7) ≈ -0.010 ½ ln(0.8) + ½ ln(1.15) ≈ -0.042 Buy-and-hold strategies cannot do better than Asset 1 in the long-term. All passive styles lose. Heads Tails Net return on active investment (1:1 mix) ½ 40% + ½ (-20%) = +10% ½ (-30%) + ½ 15% = -7.5% Growth rate of investment ½ ln(1.10) + ½ ln(0.925) ≈ +0.0087 The constant proportions strategy generates positive growth. It beats all passive styles.\n",
    "\n",
    "> The power of rebalancing strategies is often claimed to be the result of ''buying low and selling high''. However, this is the gambler's fallacy. Not does arbitrage (the opportunity to get 'something for nothing') drive this phenomenon--the market in the example is free of arbitrage. Finally, all investors have equal opportunities, unlike in Parrondo's paradox (Harmer and Abbott 1999) where some investors (depending on their wealth) are given favorable odds, and this excludes the dynamics of the wealth distribution as an explanation. The engine that generates growth from volatility is in fact an elementary mathematical relation, the Jensen inequality. It describes the effect of interchanging concave (or convex) functions and weighted averages, here portfolio asset proportions. Constant proportion rebalancing strategies combine random returns in fixed proportions at each portfolio rebalance. The expected logarithm, i.e. growth rate, of this financially engineered return is higher than the combination of the assets' individual growth rates in the same proportions, because the logarithm is a strictly concave function. For fixed, deterministic returns, both growth rates are equal, and no excess growth can be achieved.\n",
    "\n",
    "Their experiment assumed that **Stock A** would either gain +40% or lose -30% at each period and **Stock B** would either lose -20% or gain +15% at each period (randomly).\n",
    "\n",
    "![](C:\\Users\\luca\\Google Drive\\NoViews\\Graphs\\demo.png)\n",
    "\n",
    "Sometimes the world can be paradoxical. Imagine a asset which is doubling or halving at each step.\n",
    "\n",
    "Unfortunately, there is no formula to compute the edge in this case (i.e. where you are NOT losing everything in the case of a bad outcome)\n",
    "\n",
    "Most applicable in Bonds\n",
    "\n",
    "#### Ergodic Property\n",
    "\n",
    "#### Unbiased estimator\n",
    "\n",
    "The estimator $T = t(X1;X2; : : : ;Xn)$ of $\\theta$ is said to be unbiased if $E_{\\theta}(T) = \\theta$. If an estimator is biased we define the Bias to be:\n",
    "\n",
    "$$B_{\\theta}(T) = \\theta$ = E_{\\theta}(T) - \\theta$$\n",
    "\n",
    "In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors or deviations--that is, the difference between the estimator and what is estimated. MSE is a risk function, corresponding to the expected value of the squared error loss or quadratic loss. The difference occurs because of randomness or because the estimator doesn't account for information that could produce a more accurate estimate.\n",
    "\n",
    "#### Markov Inequality\n",
    "\n",
    "**Positive** RV, finite first moment ($E(X) < ∞$); it states that:\n",
    "\n",
    "$\\Pr(X \\geq a) \\leq E(x)/a$\n",
    "\n",
    "i.e. the probability of a RV of being greater than the constant c should be less or equal to the expected value divided by c. _The intuition is that, if this doesn't hold, the expected value couldn't have the value it takes._ Here the [**video**](https://www.youtube.com/watch?v=uh-v7LchsxU). Useful only on the upper bound. Remembering that the RV should be positive greatly helps in the intuitive explanation of the inequality.\n",
    "\n",
    "#### Chebyshev Inequality\n",
    "\n",
    "If I also know the variance of the RV, and I know that it is not infinite, I can get a better bound compared to Markov Inequality.\n",
    "\n",
    "$\\Pr(|X-\\mu |\\geq k\\sigma )\\leq {\\frac {1}{k^{2}}}$\n",
    "\n",
    "The proof trasforms the left argument by squaring $(x - \\mu)$ and $k$, then simply applies Markov Inequality.\n",
    "\n",
    "#### Chernoff Bound\n",
    "\n",
    "The [Chernoff Bound](http://math.mit.edu/~goemans/18310S15/chernoff-notes.pdf)\n",
    "\n",
    "#### Geometric Distribution\n",
    "\n",
    "In probability theory and statistics, the geometric distribution is either of two discrete probability distributions:\n",
    "\n",
    "- The probability distribution of the number X of Bernoulli trials needed to get one success, supported on the set { 1, 2, 3, ...}\n",
    "- The probability distribution of the number Y = X − 1 of failures before the first success, supported on the set { 0, 1, 2, 3, ... }\n",
    "\n",
    "#### Maxima Distribution\n",
    "\n",
    "- Taleb's [tweet](https://twitter.com/nntaleb/status/892553646738722816) (see pag. 254 Embrechts)\n",
    "\n",
    "- **test of independence** for maxima.\n",
    "- Harmonic number\n",
    "\n",
    "#### Log-log vs semi-log plot\n",
    "\n",
    "- All equations of the form: $y = a^x$ form a straight lines when plotted semi-logarithmical graph. Taking the log on both sides, \n",
    "  $$\n",
    "  log_{a}(y) = log_{a}(a^x)\n",
    "  $$\n",
    "  hence, $log_{a}(y) = x + log(c)$; setting $log(c) = k$, we obtain $log_{a}(y) = x + k$; then is a linear function of $x$\n",
    "\n",
    "- Relations of the form $y= x^k$ appear as straight lines in a log–log graph. Taking the log on both sides, \n",
    "  $$\n",
    "  \\log(y) = \\log(x^\\alpha)\n",
    "  $$\n",
    "  hence, $\\log(y) = \\alpha \\log(x)$; setting $\\log(y) = Y$ and $\\log(x) = X$, we obtain $Y = \\alpha X$\n",
    "\n",
    "#### Weak and Strong Law of Large Number\n",
    "\n",
    "[Find a satisfying definition]\n",
    "\n",
    "#### Stable Distribution\n",
    "\n",
    "A non-degenerate distribution is a **stable distribution** if it satisfies the following property: let $X_{1}$ and $X_{2}$ be independent copies of a random variable $X$. Then $X$ is said to be stable if for any constants $a > 0$ and $b > 0$ the random variable $aX_{1} + bX_{2}$ has the same distribution as $cX + d$ for some constants $c > 0$ and $d$. Since the normal distribution, the Cauchy distribution, and the Lévy distribution all have the above property, it follows that they are special cases of stable distributions. (_Location-Scale Family: is a family of probability distributions parametrized by a location parameter and a non-negative scale parameter. For any random variable X whose probability distribution function belongs to such a family, the distribution function of $Y = a +bX$ also belongs to the family_).\n",
    "\n",
    "#### Bayes Theorem\n",
    "\n",
    "the denominator can be thought as: $P(B | A) \\cdot P(A) + P(B |\\overline{A}) \\cdot P(\\overline{A})$; the numerator is the first element of this sum. Probability of $A$ given $B$ can be thought as of which share of the probability of $B$ is made of the probability of $A$ given that $B$ happened (further explain).\n",
    "\n",
    "#### Dot Product\n",
    "\n",
    "In mathematics, the dot product or scalar product is an algebraic operation that takes two equal-length sequences of numbers and returns a single number. Algebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers. It is the numerator (demeaned) in the covariance computation. Common also to Cosine similarity, regression coefficient and Pearson correlation (same numerator, see article\n",
    "\n",
    "#### Exponential vs Power Law\n",
    "\n",
    "- Exponential: $y = x^k$\n",
    "- Power law: $y = k^x$\n",
    "\n",
    "That's the difference. As for \"looking the same\", they're pretty different: Both are positive and go asymptotically to 00, but with, for example y=(1/2)x, the value of y actually cuts in half every time x increases by , whereas, with y=x−2, notice what happens as x increases from 1 million to 1 million+1\\. The amount by which y gets multiplied is barely less than 1, and if you put \"billion\" in place of \"million\", then it's even closer to 1\\. With the exponential function, it always gets multiplied by 1/2 no matter how big x gets. Also, notice that with the exponential probability distribution, you have the property of **memorylessness**.\n",
    "\n",
    "#### Memorylessness\n",
    "\n",
    "In probability and statistics, memorylessness is a property of certain probability distributions. It usually refers to the cases when the distribution of a **\"waiting time\"** until a certain event, does not depend on how much time has elapsed already. Only two kinds of distributions are memoryless: exponential distributions of non-negative real numbers and the geometric distributions of non-negative integers. In contrast, let us examine a situation which would exhibit memorylessness. Imagine a long hallway, lined on one wall with thousands of safes. Each safe has a dial with 500 positions, and each has been assigned an opening position at random. Imagine that an eccentric man walks down the hallway, stopping once at each safe to make a single random attempt to open it. In this case, we might define random variable X as the lifetime of his search, expressed in terms of \"number of attempts the man must make until he successfully opens a safe\". In this case, $E[X]$ will always be equal to the value of 500, regardless of how many attempts have already been made. Real-life examples of memorylessness include the time until a given radioactive particle decays, the time until the discovery of a new Bitcoin block, and (over the short term) the time a storekeeper must wait before the arrival of their next customer.\n",
    "\n",
    "Suppose X is a discrete random variable whose values lie in the set ${ 0, 1, 2, ... }$. The probability distribution of $X$ is memoryless precisely if for any $m$, $n$ in ${ 0, 1, 2, ... }$, we have\n",
    "\n",
    "$\\Pr(X>m+n | X>m)=\\Pr(X>n)$\n",
    "\n",
    "Here, $Pr(X>m+n|X>m)$ denotes the conditional probability that the value of X is larger than m + n, given that it is larger than or equal to m.\n",
    "\n",
    "#### Borel Cantelli Lemma\n",
    "\n",
    "Let ${E_{n}}$ be a sequence of events. Each event is a collection of outcomes. The limit superior of the collection ${En}$ is the collection of all those outcomes that appear in infinitely many events. The Borel Cantelli Lemma says that if the sum of the probabilities of the ${En}$ are finite, then the collection of outcomes that occur infinitely often must have probability zero. To give an example, suppose I say that I have a favorite real number between 0 and 1, and I challenge my friends to guess a subset of [0,1] where this number lies. Of course, I have infinitely many friends (not implausible), so they each makes a guess {En}. Each guess {En} is a collection of numbers, which here represent the outcomes. Now, a probabilist comes along assigns to each friend the probability they were correct. Thus, if a friend chose [0,1/2], they would have a 1/21/2 probability of being correct, etc. Now, this probabilist adds together all these probabilities, and gets a finite number. What can he conclude? That the collection of all those numbers that infinitely many friends guessed (the lim sup) had a 00 percent chance of being correct, so no infinite collection of friends could agree on more than a few numbers in their collection.\n",
    "\n",
    "#### Normal Distribution Popularity\n",
    "\n",
    "It is the most common distribution in nature (as distributions go). An enormous number of statistical relationships become clear and tractable if one assumes the normal. The normal (Gaussian) distribution is to statistics what Newtonian mechanics is to physics. Sure, nothing in real life exactly matches the Normal. But it is uncanny how many things come close. And this is partly due to the Central Limit Theorem, which says that if you average enough unrelated things, you eventually get the Normal.\n",
    "\n",
    "#### Characteristic Function\n",
    "\n",
    "The [characteristic function](https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)) provides an alternative way for describing a random variable. Similar to the cumulative distribution function, which is equal to $1$ when $X ≤ x$, and zero otherwise, which completely determines behavior and properties of the probability distribution of the random variable $X$, the characteristic function also completely determines behavior and properties of the probability distribution of the random variable $X$. If a random variable admits a probability density function, then the characteristic function is the _Fourier transform_ of the probability density function.\n",
    "\n",
    "#### Chernoff Bound\n",
    "\n",
    "<span style=\"color:#27916E\">\n",
    "  <strong>Here the Python <a href=\"https://github.com/lucaberlanda/Magazzino/blob/master/Stats%26Probability/ChernoffBound.py\">code</a></strong>\n",
    "</span>\n",
    "\n",
    ". In probability theory, the Chernoff bound gives exponentially decreasing bounds on tail distributions of sums of independent random variables. It is a sharper bound than the known first or second moment based tail bounds such as Markov's inequality or Chebyshev inequality, which only yield _power-law bounds on tail decay_. However, the Chernoff bound requires that the variates be independent – a condition that neither the Markov nor the Chebyshev inequalities require.\n",
    "\n",
    "#### Atomic Payoff\n",
    "\n",
    "In statistics, for nonnegative values of x, the error function has the following interpretation: for a random variable X that is normally distributed with mean 0 and variance 1/2, erf(x) describes the probability of X falling in the range [−x, x].\n",
    "\n",
    "#### Renormalization\n",
    "\n",
    "In quantum field theory, the statistical mechanics of fields, and the theory of self-similar geometric structures, renormalization is any of a collection of techniques used to treat infinities arising in calculated quantities\n",
    "\n",
    "#### Power Spectrum\n",
    "\n",
    "The power spectrum of a time series $x(t)$ describes how the variance of the data $x(t)$ is distributed over the frequency domain, into spectral components which the series $x(t)$ may be decomposed. In other words, it shows at which frequencies variations are strong and at which frequencies variations are weak.\n",
    "\n",
    "#### Probability Approaches\n",
    "\n",
    "The traditional 'frequentist' methods which use only sampling distributions are usable and useful in many particularly simple, idealized problems; however, they represent the most prescribed special cases of probability theory, because they presuppose conditions (independent repetitions of a 'random experiment' but no relevant prior information) that are hardly ever met in real problems. This approach is quite inadequate for the current needs of science.\n",
    "\n",
    "#### Threshold Effect\n",
    "\n",
    "A common error, when judging the effects of radioactivity or the toxicity of some substance, is to assume a linear response model without threshold (i.e. without a dose rate below which there is no ill effect). Presumably there is no threshold effect for cumulative poisons like heavy metal ions (mercury, lead), which are eliminated only very slowly, if at all. But for virtually every organic substance (such as saccharin or cyclamates), the existence of a finite metabolic rate means that there must exist a finite threshold dose rate, below which the substance is decomposed, eliminated, or chemically altered so rapidly that it causes no ill effects. If this were not true, the human race could never have survived to the present time, in view of all the things we have been eating. Indeed, every mouthful of food you and I have ever taken contained many billions of kinds of complex molecules whose structure and physiological effects have never been determined – and many millions of which would be toxic or fatal in large doses [...]. If we hope to detect any phenomenon, we must use a model that at least allows the possibility that it may exist.\n",
    "\n",
    "#### Influence Function\n",
    "\n",
    "Much a statistical functional changes in response to arbitrary change in a single data point, that is, replace the i-th value in the sample by an arbitrary value and consider at the output of the estimator.\n",
    "\n",
    "#### Convolution\n",
    "\n",
    "The convolution of probability distributions arises in probability theory and statistics as the operation in terms of probability distributions that corresponds to the addition of independent random variables and, by extension, to forming linear combinations of random variables. The operation here is a special case of convolution in the context of probability distributions.\n",
    "\n",
    "\n",
    "\n",
    "### Self Organized Criticality (SOC)\n",
    "\n",
    "(_How Nature Works - Per Bak_) - Criticality in the sandpile experiments means avalanches. Self organized in the sense that the avalanches happen according to power laws.\n",
    "\n",
    "### Punctuated Equilibria\n",
    "\n",
    "(_How Nature Works - Per Bak_) - Punctuated Equilibria: evolution is not smooth, it is the alternation of stasis and sudden changes (see inflation). Fitness Landscape: evolve to stay where you are in the landscape (see competition).\n",
    "\n",
    "### Induction\n",
    "\n",
    "È quel procedimento che dall'osservazione dei casi particolari passa alla legge universale e che è l'inverso della deduzione. Il problema del fondamento dell'induzione può essere formulato così: che cosa autorizza a universalizzare e affermare per tutta la classe ciò che si è verificato solo in alcuni individui o casi di essa?\n",
    "\n",
    "SILENT RISK\n",
    "\n",
    "Binaries\n",
    "\n",
    "Statistical distributions that belong to the thin-tailed domain include: Gaussian, Binomial, Bernoulli, Poisson, Gamma, Beta and Exponential. In fat tailed domains of risk harm comes from the largest single event. Examples of relevant statistical distributions include: Pareto, Levy-Stable distributions with infinite variance, Cauchy, and power law distributions, especially with larger exponents. The binary has necessarily a thin-tailed distribution, regardless of domain. We can assert the following:\n",
    "\n",
    "The sum of binaries converges at a speed faster or equal to that of the variable. The sum of binaries is never dominated by a single event, while that of the variable can be (see my simulation on Cauchy). There are serious statistical differences between predictions, bets, and exposures that have a yes/no type of payoff, the \"binaries\", and those that have varying payoffs, which we call the \"variable\". Real world exposures tend to belong to the variable category, and are poorly captured by binaries. Yet much of the economics and decision making literature confuses the two.\n",
    "\n",
    "Variable exposures are sensitive to Black Swan effects, model errors, and prediction problems, while the binaries are largely immune to them. The binaries are mathematically tractable, while the variable are much less so. Hedging variable exposures with binary bets can be disastrous–and because of the human tendency to engage in attribute substitution when confronted by difficult questions,decision-makers and researchers often confuse the variable for the binary. (<http://www.stat.berkeley.edu/> aldous/157/Papers/talebtetlock.pdf)\n",
    "\n",
    "Binaries always belong to the class of thin-tailed distributions, because of boundedness, while the variables don't. This means the law of large numbers operates very rapidly there. Extreme events wane rapidly in importance: for instance, as we will see further down in the discussion of the Chernoff bound, the probability of a series of 1000 bets to diverge more than 50 the expected average is less than 1 in 101\\. The research literature documents a certain class of biases, such as \"dread risk\" or \"long shot bias\", which is the overestimation of some classes of rare events, but derived from binary variables, then extends the result to variable exposures (Barberis, 2013). Such extension is mathematically incorrect, and leads to risk-bearing policies that do not match the research. If ecological exposures in the real world tends to have variable, not binary properties, then many results are invalid; this paper will provide a framework to compare the two.\n",
    "\n",
    "### Long Shot Bias\n",
    "\n",
    "In gambling and economics, the favourite-longshot bias is an observed phenomenon where on average, bettors tend to overvalue \"long shots\" and undervalue favourites. That is, in a horse race where one horse is given odds of 2-to-1, and another 100-to-1, the true odds might for example be 1.5-to-1 and 300-to-1 respectively. Betting on the \"long shot\" is therefore a much worse proposition than betting on the favourite. In the long run, losing 5 percent by betting on the favourite, but losing 40 percent on longshots is not uncommon. Various theories exist to explain why people willingly bet on such losing propositions, such as risk-loving behavior, risk-averse behavior or simply inaccurate estimation.\n",
    "\n",
    "### Fat Tails\n",
    "\n",
    "The problem of fat tails is usually misunderstood. It does not mean more volatility, but that a larger share of the properties comes from a small number of events; their \"impact\" gets larger and larger and more and more unpredictable. It is important to note that, ironically, not only do Black Swan effects (i.e., highly unpredictable events of large magnitude) not impact the binary predictions, but they even make them more mathematically tractable, as will see further down.\n",
    "\n",
    "In Fooled by Randomness, the narrator is asked \"do you predict that the market is going up or down?\" \"Up\", he said, with confidence. Then the questioner got angry when he discovered that the narrator was short the market, i.e., would benefit from the market going down. The trader had a difficulty conveying the idea that someone could hold the belief that the market had a higher probability of going up, but that, should it go down, it would go down a lot. So the rational response was to be short.\n",
    "\n",
    "Another way to put the point: to achieve the reputation of \"Savior of Western civilization,\"a politician such as Winston Churchill needed to be right on only one super-big question (such as the geopolitical intentions of the Nazis) and it matters not how many smaller errors that politician made (e.g. Gallipoli, gold standard, autonomy for India). Churchill could have a terrible Brier score (binary accuracy) and a wonderful reputation (albeit one that still pivots on historical counterfactuals).\n",
    "\n",
    "The binary has necessarily a thin-tailed distribution, regardless of domain. We can assert the following: - The sum of binaries converges at a speed faster or equal to that of the variable. - The sum of binaries is never dominated by a single event, while that of the variable can be (see my simulation on Cauchy).\n",
    "\n",
    "With psychology papers replicating less than 40 percent, dietary advice reversing after 30 years of fatphobia, macroeconomic analysis working worse than astrology, the appointment of Bernanke who was less than clueless of the risks, and pharmaceutical trials replicating at best only 1/3 of the time, people are perfectly entitled to rely on their own ancestral instinct and listen to their grandmothers (or Montaigne and such filtered classical knowledge) with a better track record than these policymaking goons.\n",
    "\n",
    "### IYI\n",
    "\n",
    "The Intellectual Yet Idiot is a production of modernity hence has been accelerating since the mid twentieth century, to reach its local supremum today, along with the broad category of people without skin-in-the-game who have been invading many walks of life. Why? Simply, in most countries, the government's role is between five and ten times what it was a century ago (expressed in percentage of GDP).\n",
    "\n",
    "Beware the semi-erudite who thinks he is an erudite. He fails to naturally detect sophistry (the use of reasoning or arguments that sound correct but are actually false)\n",
    "\n",
    "### War and Conflicts\n",
    "\n",
    "Historiography, in a way compatible with statistical inference, which needs to accommodate the fat-tailedness of the data and the unreliability of the reports of conflicts. We investigate the theses of \"long peace\" and drop in violence and find that these are statistically invalid and resulting from flawed and naive methodologies, incompatible with fat tails and non-robust to minor changes in data formatting and methodologies. There is no statistical basis to claim that \"times are different\" owing to the long inter-arrival times between conflicts; there is no basis to discuss any \"trend\", and no scientific basis for narratives about change in risk.\n",
    "\n",
    "For a thin tailed process, probability swamps payoff. Conditional of exceeding a value K (in absolute value), when K is large, the conditional expectation of x is about K. For a fat tailed process the expectation will be a multiple of K no matter how large K is. Details are in chapter 4 as this is exactly how we define the class that is higher in the fat tails hierarchy than the subexponential class.\n",
    "\n",
    "People falling from ladders tripling next year is of the order of 10^(-14) >>> is subjected to the Chernoff bound (thin tailed process). The use of \"robust\" statistics in some domains always makes one's exposure more fragile, and vice versa. We will attempt to specify in which domains increasing statistical robustness directly worsens exposures.\n",
    "\n",
    "### Mean Absolute Deviation and ARE\n",
    "\n",
    "<span style=\"color:#27916E\"> With <strong>python</strong> code</span>\n",
    "\n",
    ". As soon as I am simulating two gaussian mix with different variances, the ARE becomes higher and higher. Say someone just asked you to measure the \"average daily variations\" for the temperature of your town (or for the stock price of a company, or the blood pressure of your uncle) over the past five days. The five changes are: (-23, 7, -3, 20, -1). How do you do it? STD gives more weight to outliers, is less robust.\n",
    "\n",
    "1) MAD is more accurate in sample measurements, and less volatile than STD since it is a natural weight whereas standard deviation uses the observation itself as its own weight, imparting large weights to large observations, thus overweighing tail events.\n",
    "\n",
    "2) We often use STD in equations but really end up reconverting it within the process into MAD (say in finance, for option pricing). In the Gaussian world, STD is about 1.25 time MAD, that is. But we adjust with stochastic volatility (i.e. if we fatten the tail of the distribution of returns) where STD is often as high as 1.6 times MAD.\n",
    "\n",
    "3) Many statistical phenomena and processes have \"infinite variance\" (sa the popular Pareto 80/20 rule) BUT HAVE FINITE, AND VERY WELL BEHAVED, MEAN DEVIATIONS. Whenever the mean exists, MAD exists. The reverse (infinite MAD and finite STD) is never true.\n",
    "\n",
    "4) Many economists have dismissed \"infinite variance\" models thinking these meant \"infinite mean deviation\". Sad, but true. When the great Benoit Mandelbrot proposed his infinite variance models fifty years ago, economists freaked out because of the conflation.\n",
    "\n",
    "It is sad that such a minor point can lead to so much confusion: our scientific tools are way too far ahead of our casual intuitions, which starts to be a problem with science. So I close with a statement by Sir Ronald A. Fisher: 'The statistician cannot evade the responsibility for understanding the process he applies or recommends.'\n",
    "\n",
    "A more advanced point that we will develop progressively as we define thintailedness and map the origin of both fat and thin tails. TAKE FOR NOW THAT, IN GENERAL, IN NATURE, BECAUSE F (X) THE RESPONSE OF ENTITIES AND ORGANISMS TO RANDOM EVENTS IS GENERALLY THINTAILED WHILE X CAN BE FAT-TAILED, OWING TO F (X) HAVING THE SIGMOID \"S\" SHAPE CONVEXCONCAVE (some type of floor below, progressive saturation above). Deeper on The MAD: h = E(X2)E(X)\n",
    "\n",
    "As long as I increase the fatness of tails, h explodes (more generally, the norm rises for higher values of p, see Taleb).\n",
    "\n",
    "Remark: A function of a random variable, s.a. exposure, needs to be treated as a separate random variable.\n",
    "\n",
    "Remark\n",
    "\n",
    "In more general usage, a central limit theorem is any of a set of weak-convergence theorems in probability theory. They all express the fact that a sum of many independent and identically distributed (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of attractor distributions. When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of i.i.d. random variables with power law tail distributions decreasing as |x|−α−1 where 0 < α < 2 (and therefore having infinite variance) will tend to an alpha-stable distribution with stability parameter (or index of stability) of α as the number of variables grows.[3]\n",
    "\n",
    "The tail start depends on the crossover point (see more)\n",
    "\n",
    "Fat tails means where P(X > nK)/P(X > K) depends only on n and not on K (or at least that the tail asymptotically has this behaviour). For X large enough P(X > x = C x exp(-alpha), or at least C tends to a constant with x → ∞. We refer to these distributions as TRUE fat tails, to distinguish them from distributions that only have a higher kurtosis than the Gaussian\n",
    "\n",
    "Some define fat tails as infinite variance (define it together with infinite mean) distributions.\n",
    "\n",
    "Sub-exponential distributions: (1 - F2(x)) / (1 - F(x)) = 2 for x → ∞. Where F_(x) = to the convolution of x with itself. The result can be extended in this way: (1 - Fn(x)) / (1 - F(x)) = n for x → ∞ (verbose definition: the probability that X exceeds the sum of two (n) rv on the probability that X exceeds one rv is 2 (n) >> SIMULATE with Cauchy)._\n",
    "\n",
    "Fatter and fatter tails: different values for a. Note that higher peak implies a lower probability of leaving the 1 st dev tunnel, making the probability of extreme events drop, but their contribution paradoxically increases.\n",
    "\n",
    "Useful links <http://www.fooledbyrandomness.com/pp2.pdf> (Precautionary Principle)\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "REFERENCES:\n",
    "\n",
    "Handbook of Montecarlo methods: <http://www.maths.uq.edu.au/~kroese/montecarlohandbook/>\n",
    "\n",
    "Bayesian Analysis for a Logistic Regression Model: <http://it.mathworks.com/help/stats/examples/bayesian-analysis-for-a-logistic-regression-model.html#zmw57dd0e4688>\n",
    "\n",
    "Bayesian Inference <http://www-math.bgsu.edu/~albert/m648/m648_old2.html>\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Intellectual and ethical freedom requires the absence of the skin of others in one's game; but neither celibacy nor financial independence make one unconditionally immune.\n",
    "\n",
    "Far-fetched comparisons are more likely to discredit the commentator than the commentated.\n",
    "\n",
    "Hills estimator asymptotically normal\n",
    "\n",
    "[]()\n",
    "\n",
    "# Extreme Value Theory\n",
    "\n",
    "Q Is the exponent usually underestimated?\n",
    "\n",
    "Let's start with a Taleb's note:\n",
    "\n",
    "> Extreme Value Theory has been considered a panacea for dealing with extreme events by a bunch of \"risk modelers\". On paper it looks great. But only on paper. The problem is the calibration and parameter uncertainty --in the real world we don't know the parameters. The ranges in the probabilities generated we get are monstrous. This is a short presentation of the idea, followed by an exposition of the difficulty.\n",
    "\n",
    "## Fisher–Tappet and Gnedenko Theorem\n",
    "\n",
    "The main theorem of EVT is usually compared to the central limit theorem of probability. As the central limit theorem states the convergence of the standardized sum of any sequence of iid random variables (satisfying some technical assumptions) to a given distribution (the normal), so the results from Fisher, Tippett and Gnedenko guarantee that the standardized maximum of a sequence of iid random variables (again, under technical assumptions) converges to some given distribution family (Gumbel, Fréchet, Weibull). However, while the central limit theorem deals with sums of random variables, EVT focuses on maxima. Moreover, the central limit theorem provides a unique limit distribution, while EVT includes three different families of asymptotic distributions.\n",
    "\n",
    "F is said to be in the maximum domain of attraction of a Fréchet distribution the upper tail of F decays as a power function, multiplied by a slowly varying function. The Fréchet extreme value distribution is particularly important to finance, since most financial time-series are fat-tailed, thus displaying an asymptotic distribution of extremes that is of Fréchet type. The parameter α := 1/? > 0 is known as the tail index and is directly related to the tail behaviour of the distribution. For instance, one can prove that:\n",
    "\n",
    "## Hill's estimator\n",
    "\n",
    "<span style=\"color:#27916E\"> With <strong>python</strong> code</span>\n",
    "\n",
    ". The Hill's estimator is defined as follows:\n",
    "\n",
    "Hills estimator can be defined as the empirical mean excess function. We can see in the graph how it behaves. Employing graphical methods (known as Hill plots when the Hill estimator is used), displaying the estimated values of as a function of the cut-off, in order to find out some interval of candidate cut-off points that yield stable estimates of  (corresponding to a horizontal line in the Hill plot). The estimator thus depends on the parameter k 2 f2; : : : ; ng, which represents the cut-off between the observations considered as belonging to the center of the distribution and those pertaining to the upper tail, so that order statistics Xj;n with 1 j < k can be considered as extreme realizations. The dependence of the Hill estimator on k is a critical issue for the application of the method to empirical studies and we will discuss it in Section 2\\. On the other hand, the Hill estimator has undergone both deep theoretical study and intensive application, displaying very good performance, competitive (and in some cases even superior) with respect to other EVT approaches. From a theoretical viewpoint, the favorable consideration19 towards the Hill estimator is justified by its asymptotic properties:\n",
    "\n",
    "- Weak consistency\n",
    "- Strong consistency\n",
    "- Asymptotic normality – under additional hypotheses\n",
    "\n",
    "**Graph 1 – Hill plot for t-student and Gaussian distributions** Gaussian and standard-t distributions. The lower the indicator, the lower the estimated alpha, the fatter the tails. In the student t case, the alpha is equal to its degrees of freedom. _Why in the graph, the two lines are declining that way?_\n",
    "\n",
    "![](C:\\Users\\Utente\\Google Drive\\NoViews\\Graphs\\EVT1.png)\n",
    "\n",
    "**Graph 2 - Mean Exceedances Plot of an ETF** Mean Exceedances plot of the Amundi ETF MSCI Emerging Markets UCITS ETF – EUR.\n",
    "\n",
    "![](C:\\Users\\Utente\\Google Drive\\NoViews\\Graphs\\EVT2.png)\n",
    "\n",
    "**Graph 3 – Log-log Plot of an ETF** Log-log plot of the Amundi ETF MSCI Emerging Markets UCITS ETF – EUR.\n",
    "\n",
    "![](C:\\Users\\Utente\\Google Drive\\NoViews\\Graphs\\EVT3.png)\n",
    "\n",
    "**Graph 4 – Hill Plot for an ETF** Hill Plot of the Amundi ETF MSCI Emerging Markets UCITS ET. We should look for the Hill plot stability (visualized as a horizontal line) in order to find out some interval of cut-off that yield stable estimates of the Shape parameter.\n",
    "\n",
    "![](C:\\Users\\Utente\\Google Drive\\NoViews\\Graphs\\EVT4.png)\n",
    "\n",
    "## Cutoff points\n",
    "\n",
    "Both parametric (?) and non-parametric extreme value methods share a common drawback: the estimates they provide are sensitive to the choice of the cut-off parameter, i.e. to the number m of blocks in the block-maxima method, the threshold u in the threshold-exceedance method, or the index k of the order statistic marking the frontier between the center and the tail of the distribution in non-parametric approaches. The choice of the cut-off parameter is thus a central issue to any application of EVT, since in principle different estimates of the shape parameter can lead to significantly different estimates of the VaR, for example.\n",
    "\n",
    "**Conventional choices of cut-off point**: many authors do not address explicitly the problem of the choice of the cut-off for the data they are handling, but simply follow either common sense choices or suggestions retrieved in the literature, usually based on one of the methods that we are presenting in the following paragraphs. A widely used suggestion in this respect is that the number of data falling in the tail should not be higher than 10-15% and a rule of thumb value of 5-10% is often used.\n",
    "\n",
    "## Mad on Standard Deviation\n",
    "\n",
    "Here we try to find an estimator for the tails in financial time series. The first indicator we analyze is: MAD (Mean Absolute Deviation) / standard deviation (SD). We can use the MAD/SD ratio as a gauge of fat-tailedness. The closer the number is to zero, the fatter the tails. The closer the number is to 1 (it can never exceed 1!), the thinner the tails (_why it cannot be > 1?_). In the figure, we plot the MAD/SD ratio for a student-t with varying degrees of freedom. We can see from the green histograms that the MAD on SD ratio is a much more robust measure of fat-tails.\n",
    "\n",
    "Kurtosis is infinite for t-student? What about the MAD SD ratio?\n",
    "\n",
    "## Kurtosis\n",
    "\n",
    "The kurtosis coefficient is often regarded as a measure of the tail heaviness of a distribution relative to that of the normal distribution. However, it also measures the peakedness of a distribution, hence there is no agreement on what kurtosis really estimates. Another disadvantage of the kurtosis is that its interpretation and consequently its use is restricted to symmetric distributions. Moreover, the kurtosis coefficient is very sensitive to outliers in the data This explains in a way why the detection of fragility is vastly more potent than that of risk –and much easier to do. We can use the past to derive general statistical statements, of course, coupled with rigorous probabilistic inference but it is unwise to think that the data unconditionally yields precise probabilities, as we discuss next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
