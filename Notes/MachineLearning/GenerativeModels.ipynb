{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "839793b8-ed17-4e0a-a898-579ee12870bb",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "Generative models model the probability distribution of each class. We will see two kind of models: **Multinomial** and **Gaussian**, and we have to answer to two order of questions: how to **estimate** the models and how to **predict** using these models. \n",
    "\n",
    "## Multinomial Models\n",
    "\n",
    "Let's take the case of documents production. In the multinomial case we want to find the probability that a certain word is chosen given the parametrization $\\theta$ of the model:\n",
    "\n",
    "$$\n",
    "\\text{Prob}(w\\vert \\theta) = \\theta_w\n",
    "$$\n",
    "\n",
    "Given that $\\theta_w \\geq 0$ and that all the $\\theta_w$s sum to one. For a document where words are independently \n",
    "generated the **likelihood of the document** is:\n",
    "\n",
    "$$\n",
    "\\text{Prob}(\\text{Doc} \\vert \\theta) = \\prod_{i=1}^n \\theta_{w_i} = \\prod_{w \\in W} \\theta_w^{\\text{count w}}\n",
    "$$\n",
    "\n",
    "The probability of a document is the product of the appearance of each word, i.e. the product of each word at the power of the number of the word's appearances in the document. What we want to do is to maximize the probability of the word being generated \"moving\" $\\theta$.\n",
    "\n",
    "$$\n",
    "\\max_{\\theta} \\prod_{w \\in W} \\theta_w^{\\text{count w}}\n",
    "$$\n",
    "\n",
    "Taking the log it yields:\n",
    "\n",
    "$$\n",
    "\\sum_{w \\in W} \\text{count}(w) \\log(\\theta_w)\n",
    "$$\n",
    "\n",
    "Taking the derivative and setting to zero we get:\n",
    "\n",
    "$$\n",
    "\\tilde{\\theta}_w = \\frac{\\text{count}(w)}{\\sum_{w\\prime \\in W} \\text{count}(w \\prime) }\n",
    "$$\n",
    "\n",
    "For a dictionary with more than 2 words we get this result by applying Lagrangian Multipliers ([check here](https://courses.edx.org/courses/course-v1:MITx+6.86x+1T2020/courseware/unit_4/lec15_gm/?activate_block_id=block-v1%3AMITx%2B6.86x%2B1T2020%2Btype%40sequential%2Bblock%40lec15_gm])). \n",
    "\n",
    "Consider using a multinomial generative model $M$ for the task of binary classification consisting of two classes which are denoted by + (positive class) and - (negative class). Let the parameters of $M$ that maximize the likelihood of training data for the positive class be denoted by $\\theta^+$ and for the negative class be denoted by $\\theta^-$.\n",
    "\n",
    "Also, suppose that we classify a new document $D$ to belong to the positive class if and only if\n",
    "\n",
    "$$\n",
    "\\log \\frac{P(D | \\theta ^+)}{P(D | \\theta ^-)} \\ge 0\n",
    "$$\n",
    "\n",
    "Which becomes:\n",
    "\n",
    "$$\n",
    "\\sum_{w \\in W} \\text{count}(w) \\log \\frac{\\theta^+}{\\theta^-} = \\sum_{w \\in W} \\text{count}(w) \\tilde{\\theta}\n",
    "$$\n",
    "\n",
    "And we can see that the formula has the same form of a **linear classifier that goes through the origin**. Now we may want to inject some conditionality on the words flow. \n",
    "\n",
    "$$\n",
    "\\text{Prob}(y=+ \\vert D) = \\frac{\\text{Prob}(D \\vert \\theta^+)\\text{Prob}(y=+)}{\\text{Prob}(D)}\n",
    "$$\n",
    "\n",
    "Probability that I assign $+$ given a document $D$ follows Bayes rule. $\\text{Prob}(y=+)$ is the prior. Hence:\n",
    "\n",
    "$$\n",
    "\\log \\frac{P(y=+|D)}{P(y=-|D)} = \\log \\frac{\\text{Prob}(D \\vert \\theta^+)\\text{Prob}(y=+)}{\\text{Prob}(D \\vert \\theta^-)\\text{Prob}(y=-)}\n",
    "$$\n",
    "\n",
    "Where ${\\text{Prob}(D)}$ cancels out. \n",
    "\n",
    "$$\n",
    "\\log \\frac{\\text{Prob}(D \\vert \\theta^+)}{\\text{Prob}(D \\vert \\theta^-)} \\log \\frac{\\text{Prob}(y=+)}{\\text{Prob}(y=-)}\n",
    "$$\n",
    "\n",
    "If we substitute $\\log \\frac{\\text{Prob}(y=+)}{\\text{Prob}(y=-)}$ with $\\theta_0$ we get\n",
    "\n",
    "$$\n",
    "\\sum_{w \\in W} \\text{count}(w) \\log \\frac{\\theta^+}{\\theta^-}  + \\theta_0= \\sum_{w \\in W} \\text{count}(w) \\tilde{\\theta} + \\theta_0\n",
    "$$\n",
    "\n",
    "And we can see that the formula has the same form of a **linear classifier with an offset parameter**, where the offset parameter is the log of the ratio of our priors. \n",
    "\n",
    "## Gaussian Generative Models\n",
    "\n",
    "A random vector $\\mathbf{X}=(X^{(1)},\\ldots ,X^{(d)})^ T\\,$ is a **Gaussian vector**, or **multivariate Gaussian or normal variable** , if any linear combination of its components is a (univariate) Gaussian variable or a constant (a “Gaussian\" variable with zero variance), i.e., if $\\alpha^TX$ is (univariate) Gaussian or constant for any constant non-zero vector $\\alpha \\in R^d$. The distribution of $\\mathbf{X}$, is completely specified by the vector mean $\\mu =\\mathbf E[\\mathbf{X}]= (\\mathbf E[X^{(1)}],\\ldots ,\\mathbf E[X^{(d)}])^ T$ and the $d×d$ covariance matrix $\\Sigma$. If it is invertible, then the pdf of $\\mathbf{X}$ is:\n",
    "$$\n",
    "\\displaystyle  \\displaystyle f_{\\mathbf{X}}(\\mathbf x) = \\frac{1}{\\sqrt{\\left(2\\pi \\right)^ d \\text {det}(\\Sigma )}}e^{-\\frac{1}{2}(\\mathbf x-\\mu )^ T \\Sigma ^{-1} (\\mathbf x-\\mu )}, ~ ~ ~ \\mathbf x\\in \\mathbb {R}^ d\n",
    "$$\n",
    "where $\\text {det}(\\Sigma )$ is the determinant of the $\\Sigma$, which is positive when $\\Sigma$ is invertible. If $\\mu=0$ and $\\Sigma $ is the identity matrix, then it is called a **standard normal random vector**.\n",
    "\n",
    "Recall that the **likelihood** of $x$ being generated from a multi-dimensional Gaussian with **all same mean** $\\mu$ and all the components being uncorrelated and having the same standard deviation $\\sigma$ is:\n",
    "$$\n",
    "P(x | \\mu , \\sigma ^2) = \\frac{1}{(2\\pi \\sigma ^2)^{d/2}} \\text {exp}(-\\frac{1}{2\\sigma ^2} \\|  x - \\mu \\| ^2)\n",
    "$$\n",
    "As expected, the result of the MLE for mean and variance is respectively (remember that the random variables have same mean and same variance)\n",
    "$$\n",
    "\\hat{\\mu } = \\frac{\\sum _{t=1}^{n} x^{(t)}}{n}\\\\\\hat{\\sigma }^2 = \\frac{\\sum _{t=1}^ n \\| x^{(t)} - \\mu \\| ^2}{nd}\n",
    "$$\n",
    "\n",
    "## Gaussian Mixture Models\n",
    "\n",
    "It is a generative model for data $x\\in R_d$ is defined by the following set of parameters:\n",
    "\n",
    "1. $K$: Number of mixture components\n",
    "2. A $d$-dimensional Gaussian $\\mathcal{N}(\\mu ^{(j)}, \\sigma _ j^2)$ for every $j=1,\\dots,K$\n",
    "3. $p_1,…,p_K$ Mixture weights\n",
    "\n",
    "Let all of the parameters of the Gaussian mixture model be collectively represented as:\n",
    "$$\n",
    "\\theta = \\left\\{ p_1, \\dots , p_ K, \\mu ^{(1)}, \\dots , \\mu ^{(K)}, \\sigma _1^2, \\dots , \\sigma _ K^2\\right\\}\n",
    "$$\n",
    "The **likelihood** of a point x in a GMM is given as\n",
    "$$\n",
    "\\displaystyle  p(\\mathbf{x} \\mid \\theta ) = \\sum _{j = 1}^ K p_ j \\mathcal{N}(\\mathbf{x},\\mu ^{(j)}, \\sigma _ j^2).\n",
    "$$\n",
    "Let $x$ be an observation obtained from the Gaussian mixture model in the following way:\n",
    "\n",
    "1. We draw which gaussian we will use according to the probabilities $p_1, \\dots, p_k$\n",
    "2. We draw from the selected Gaussian $X \\sim N(\\mu^{(j)}, \\sigma_j^2)$\n",
    "\n",
    "## EM (Expectation Maximization) Algorithm\n",
    "\n",
    "While a “hard\" clustering algorithm like k-means or k-medoids can only provide a cluster ID for each data point, the EM algorithm, along with the generative model driving its equations, can provide the posterior probability (“soft\" assignments) that every data point belongs to any cluster.\n",
    "\n",
    "\n",
    "**Process** (two gaussians)\n",
    "- Start with two randomly placed gaussians $\\mathcal{N_1}(\\mu_1, \\sigma _1^2), \\mathcal{N_2}(\\mu_2, \\sigma _ 2^2)$\n",
    "- For each point: does it look like it came from $\\mathcal{N_1}$ or $\\mathcal{N_2}$?\n",
    "- Adjust parameters of $\\mathcal{N_1}(\\mu_1, \\sigma _1^2)$ and $\\mathcal{N_2}(\\mu_2, \\sigma _2^2)$ to fit points assigned to them\n",
    "\n",
    "We observe $n$ data points $x_1,…,x_n$ in $R_d$. We wish to maximize the GMM likelihood with respect to the parameter set $\\theta = \\left\\{ p_1, \\dots , p_ K, \\mu ^{(1)}, \\dots , \\mu ^{(K)}, \\sigma _1^2, \\dots , \\sigma _ K^2\\right\\}$.\n",
    "\n",
    "Maximizing the log-likelihood $\\log (\\prod _{i=1}^ n p(\\mathbf x^{(i)} | \\theta ))$ is not tractable in the setting of GMMs. There is no closed-form solution to finding the parameter set $\\theta$ that maximizes the likelihood. The *EM algorithm* is an iterative algorithm that finds a locally optimal solution $\\tilde{\\theta}$ to the GMM likelihood maximization problem.\n",
    "\n",
    "**Initialization**: as for the initialization before the first time E step is carried out, we can either do a random initialization of the parameter set $\\theta$ (or we can employ k-means to find the initial cluster centers of the K clusters and use the global variance of the dataset as the initial variance of all the K clusters. In the latter case, the mixture weights can be initialized to the proportion of data points in the clusters as found by the k-means algorithm).\n",
    "\n",
    "**E Step**: it involves finding the posterior probability that point $x^{(i)}$ was generated by cluster $j$, for every $i=1,…,n$ and $j=1,…,K$. This step assumes the knowledge of the parameter set $\\theta $. We find the posterior using the following equation:\n",
    "$$\n",
    "\\displaystyle  p(\\text {point }\\mathbf x^{(i)}\\text { was generated by cluster }j | \\mathbf x^{(i)}, \\theta ) \\triangleq p(j \\mid i) = \\frac{p_ j \\mathcal{N}\\left(\\mathbf x^{(i)}; \\mu ^{(j)},\\sigma _ j^2 I\\right)}{p(\\mathbf x^{(i)} \\mid \\theta )}.\n",
    "$$\n",
    "\n",
    "**M Step**: The step maximizes a proxy function $\\hat{ℓ}(x(1),…,x(n)∣\\theta)$ of the log-likelihood over $\\theta$, where\n",
    "$$\n",
    "\\displaystyle  \\hat{\\ell }(\\mathbf x^{(1)},\\dots ,\\mathbf x^{(n)} \\mid \\theta ) \\triangleq \\sum _{i=1}^{n} \\sum _{j = 1}^ K p(j \\mid i) \\log \\left( \\frac{p\\left( \\mathbf x^{(i)} \\text { and } \\mathbf x^{(i)} \\text { generated by cluster }j \\mid \\theta \\right)}{p(j \\mid i)} \\right).\n",
    "$$\n",
    "This is done instead of maximizing over $\\theta$ the actual log-likelihood\n",
    "$$\n",
    "\\displaystyle  \\ell (\\mathbf x^{(1)},\\dots ,\\mathbf x^{(n)} \\mid \\theta ) = \\sum _{i=1}^{n} \\log \\left[\\sum _{j = 1}^ K p\\left( \\mathbf x^{(i)} \\text { generated by cluster }j \\mid \\theta \\right)\\right].\n",
    "$$\n",
    "Maximizing the proxy function over the parameter set $\\theta$, one can verify by taking derivatives and setting them equal to zero that\n",
    "\n",
    "\n",
    "$$\n",
    "\\displaystyle \\displaystyle  \\widehat{\\mu ^{(j)}}= \\frac{\\sum _{i = 1}^ n p (j \\mid i) \\mathbf x^{(i)}}{\\sum _{i=1}^ n p(j \\mid i)}\\\\\\displaystyle \\displaystyle  \\widehat{p _j}= \\frac{1}{n}\\sum _{i = 1}^ n p(j \\mid i),\\\\\\displaystyle \\displaystyle \\widehat{\\sigma _ j^2}= \\frac{\\sum _{i = 1}^ n p(j \\mid i) \\|  \\mathbf x^{(i)} - \\widehat{\\mu ^{(j)}} \\| ^2}{d \\sum _{i = 1}^ n p(j \\mid i)}.\n",
    "$$\n",
    "The E and M steps are repeated iteratively until there is no noticeable change in the actual likelihood computed after M step using the newly estimated parameters or if the parameters do not vary by much.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c83cb-6ed7-419d-8a44-ce47cd6a59e1",
   "metadata": {},
   "source": [
    "\n",
    "A **Markov decision process (MDP)** is defined by\n",
    "\n",
    "- a set of states $s\\in S$;\n",
    "- a set of actions $a \\in A$;\n",
    "- Action dependent transition probabilities $T(s,a,s')=P(s'|s, a)$, so that for each state $s$ and action $a$, $\\displaystyle \\sum _{s'\\in S} T(s,a,s')=1$.\n",
    "- Reward functions $R(s,a,s′)$, representing the reward for starting in state $s$, taking action $a$ and ending up in state $s′$ after one step. (The reward function may also depend only on $s$, or only $s$ and $a$.)\n",
    "\n",
    "MDPs depends on all the fourth quantities. MDPs also satisfy the **Markov property** in that the transition probabilities and rewards depend only on the current state and action, and remain unchanged regardless of the history (i.e. past states and actions) that leads to the current state.\n",
    "\n",
    "#### Utility Functions\n",
    "\n",
    "- **Final horizon** utility: $\\text{U}(s_o, \\dots, s_{n+k}) = \\text{U}(s_o, \\dots, s_{n})$, i.e. utility is calculated up to step $n$, then is stays the same till the last step $k$. This definition of utility though invalidates the dependency of decisions only on the current states; decision will also depend on \"timing\", i.e. if I have only one step to I will be pushed to behave in a riskier way. \n",
    "\n",
    "- **Discounted reward** utility: $U[s_0,s_1,\\ldots ]= \\sum _{k=0}^{\\infty } \\gamma ^ k R(s_ k).$ This means that we discount rewards by a factor $\\gamma$. This kind of utility is bounded to:\n",
    "  $$\n",
    "  U[s_0,s_1,\\ldots ]= \\sum _{k=0}^{\\infty } \\gamma ^ k R(s_ k) \\leq  \\sum _{k=0}^{\\infty } \\gamma ^ k R_{\\text{max}} \\leq R_{\\text{max}} \\frac{1}{1 - \\gamma} \\text {where }0\\leq \\gamma <1.\n",
    "  $$\n",
    "\n",
    "#### Policy\n",
    "\n",
    "Given an MDP, and a utility function $\\text{U}[s_0,s_1,…,s_n]$ our goal is to find an optimal policy function that maximizes the expectation of the utility. Here, a **policy** is a function $\\pi :S \\rightarrow A$ that assigns an action $\\pi(s)$ to any state $s$. We denote the optimal policy by $\\pi^*$.\n",
    "\n",
    "#### Bellman Equations\n",
    "\n",
    "- We then have the optimal policy $\\pi^*$ defined above\n",
    "\n",
    "- We define the **value function** $V^*(s)$ the expected reward at $s$ acting optimally \n",
    "\n",
    "- Then, we then define the **Q-function** $Q^∗(s,a)$ as the expected reward from starting at state $s$, then acting with action $a$, and acting optimally afterwards.\n",
    "\n",
    "  The **Bellman equations** put these three definitions together:\n",
    "  \n",
    "  $$\n",
    "  V^*(s) = \\displaystyle  \\displaystyle \\max _ a Q^*(s, a) = Q^*(s, \\pi^*(s))\\\\\n",
    "  \\displaystyle Q^*(s, a) = \\displaystyle  \\sum _{s'} T(s, a, s') (R(s, a, s') + \\gamma V^*(s') )\\\\\n",
    "  V^*(s) = \\displaystyle  \\displaystyle \\max _ a \\bigg[ \\sum _{s'} T(s, a, s') (R(s, a, s') + \\gamma V^*(s'))\\bigg]\n",
    "  \\\\\n",
    "  V^*(s) = \\displaystyle  \\displaystyle \\max _ a \\bigg[ \\sum _{s'} T(s, a, s') (R(s, a, s') + \\gamma \\max _ a Q^*(s', a') \\bigg]\n",
    "  $$\n",
    "\n",
    "The first equation means: select the set of actions that gives me the higher $Q$. The second equation means: the optimal $Q$ depends on the next step reward (weighted by its particular states with transition function) plus the optima value function discounted. \n",
    "\n",
    "If we consider the value function as the optimal reward function after $k$ steps, the  last equation becomes:\n",
    "$$\n",
    "V^*_{k+1}(s) = \\displaystyle  \\displaystyle \\max _ a \\bigg[ \\sum _{s'} T(s, a, s') (R(s, a, s') + \\gamma V^*_k(s'))\\bigg]\n",
    "$$\n",
    "And, as $k$ goes to $\\infty$,  $V^*_k(s)$ is supposed to converge to $V^*(s)$\n",
    "\n",
    "1. Initialization of the algorithm with $V^*_0(i)=0$ for every $i$\n",
    "2. Iterate until $V^*_k(s)= V^*_{k+1}(s)$ for every $s$\n",
    "\n",
    "\n",
    "\n",
    "In the real world, usually we know the actions and the states, but not the **transition probabilities** and the **rewards** which are **unknown**.  Then we should try to estimate transition probabilities:\n",
    "$$\n",
    "\\displaystyle \\displaystyle  \\hat{T} =  \\frac{\\text {count}(s, a, s')}{\\displaystyle \\sum _{s'} \\text {count}(s, a, s')}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\displaystyle \\hat{R} = \\frac{\\displaystyle \\sum _{t=1}^{\\text {count}(s, a, s')} R_ t(s, a, s')}{\\text {count}(s, a, s')}\n",
    "$$\n",
    "\n",
    "The issue with this approach (called **model based** approach) is that certain states might not be visited at all while collecting the statistics, or certain states might be visited much less often than others leading to very noisy estimates. We then try to use another approach (**model free** approach) which, instead of estimating the probability of an even occurring, simply adds up the outcomes and divides by the number of trials:\n",
    "$$\n",
    "\\displaystyle \\frac{\\sum _{i=1}^ K f(X_ i)}{K}\n",
    "$$\n",
    "**Sampling Based Approach for Q-learning**\n",
    "\n",
    "We decide to run different trials to estimate our $Q$:\n",
    "$$\n",
    "\\displaystyle \\text{sample}_1 = R(s, a, s'_1) + \\gamma \\max _ a Q^*(s'_1, a)\\\\ \\vdots \\\\\t\n",
    "\\displaystyle \\text{sample}_k = R(s, a, s'_k) + \\gamma \\max _ a Q^*(s'_k, a)\n",
    "$$\n",
    "Instead of taking a normal average of all the $k$ samples, we take an exponential running average. The algorithm becomes:\n",
    "\n",
    "1. Initializing $Q(s, a)$ with zero $\\forall s, a$\n",
    "\n",
    "2. Iterate until convergence:\n",
    "\n",
    "   1. collect sample $s, a, s'$ and $R(s, a, s')$\n",
    "\n",
    "   2. $Q_{i+1}(s) = \\displaystyle \\alpha (R(s, a, s'_i) + \\gamma \\max _ a Q^*(s'_i, a))+ (1-\\alpha)Q_i(s, a)$\n",
    "\n",
    "      $Q_{i+1}(s) = Q_i(s, a) + \\alpha (R(s, a, s'_i) + \\gamma \\max _ a Q^*(s'_i, a)-Q_i(s,a))$\n",
    "\n",
    "The second formula in step 2.2 is similar to the gradient descend one, and convergence can be demonstrated starting from there. \n",
    "\n",
    "**Exploration vs Exploitation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e6184-b1ab-483c-a15c-59c29a9d5a8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
