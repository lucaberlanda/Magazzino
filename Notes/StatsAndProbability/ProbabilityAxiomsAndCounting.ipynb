{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Axioms and Counting\n",
    "\n",
    "## Probability Axioms\n",
    "\n",
    "- Sample space ${\\displaystyle \\Omega}$ should be: mutually exclusive, collectively exhaustive, with the right granularity\n",
    "- ${\\displaystyle P(A)>= 0}$\n",
    "- ${\\displaystyle P(\\Omega)=1}$\n",
    "- ${\\displaystyle P(A ∩ \\Omega)= {P(A)}}$\n",
    "- ${\\displaystyle P(A) + P(A^c) = 1}$\n",
    "- if $A \\subseteq B$ then $P(A) \\leq P(B)$\n",
    "- ${\\displaystyle P(A ∪ B) = P(A) + P(B) - P(A ∩ B)}$\n",
    "- ${\\displaystyle P(A ∩ B^c) = P(A) - P(A ∩ B)}$\n",
    "- ${\\displaystyle S ∩ (T ∪ U) = (S ∩ T) ∪ (S ∩ U)}$\n",
    "- ${\\displaystyle S ∪ (T ∩ U) = (S ∪ T) ∩ (S ∪ U)}$\n",
    "- Bonferroni Inequality: ${\\displaystyle P(A_1 ∩ A_2) \\geq P(A_1) + P(A_2)}$\n",
    "- [De Morgan Laws](https://brilliant.org/wiki/de-morgans-laws/)\n",
    "\n",
    "## Conditioning Rules\n",
    "\n",
    "### Multiplication Rules\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(A ∩ B) &= P(B)P(A | B) \\\\\n",
    "         &= P(A)P(B | A) \\\\\n",
    "         &= P(A ∩ B ∩ C) = P(A)P(B | A)P(C | A ∩ B)\n",
    "\\end{align} \n",
    "$$\n",
    "\n",
    "### Total Probability Theorem\n",
    "\n",
    "i.e. probability of an event $A$ is the sum of the probabilities of that event happening under every possible scenario $B_n$ times the probability of that scenario happening (the same applies for **expectations**):\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\Pr(A)=\\sum _{n}\\Pr(A\\cap B_{n})}\\\\\n",
    "{\\displaystyle \\Pr(A)=\\sum _{n}\\Pr(A\\mid B_{n})\\Pr(B_{n})}\\\\\n",
    "{\\displaystyle \\Pr(A)=\\sum_{n}P(A\\mid B_{n})\\Pr(B_{n}),}\\\\\n",
    "{\\displaystyle E[X]=\\sum_{n}P(A_n)E[X\\mid A_{n}]}\\\\{\\displaystyle E[X]=\\sum_{y}p_Y(y)E[X\\mid Y = y]}\n",
    "$$\n",
    "\n",
    "### Bayes Rules\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}}}$$\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B ∩ A)}{P(B)}}}$$\n",
    "\n",
    "### Independence\n",
    "\n",
    "$${\\displaystyle P(A | B) = P(A)}$$\n",
    "\n",
    "$${\\displaystyle P(A ∩ B)=P(A)P(B)}$$\n",
    "\n",
    "- $A$ and $B$ independent ⇒ $A$ and $B^c$ independent ⇒ $B^c$ and $A$ independent ⇒ $B^c$ and $A^c$ independent\n",
    "- Independent events $≠$ Disjoint events!\n",
    "- Independence **does not imply** conditional independence\n",
    "- **Independence intuitive definition:** info of some event does not change probabilities of the other event.\n",
    "\n",
    "\n",
    "## Counting\n",
    "- **Counting Principle**: product for $i$ that goes from $1$ to $n$\n",
    "- **Permutations**: number of ways of ordering $n$ elements: $n!$\n",
    "- **Number of all possible subsets** of size ${n} \\rightarrow 2^n$\n",
    "- **Combinations**: number of $k$ elements subsets of a given $n$ elements set<br>\n",
    "\n",
    "  $${\\displaystyle \\binom{n}{k} = {\\frac {n!}{(n - k)!k!}}}$$\n",
    "\n",
    "\n",
    "- **Binomial probabilities**: probability of obtaining $k$ heads in $n$ tosses is:<br>\n",
    "\n",
    "  $$P(X=k) = \\binom{n}{k}p^k(1-p)^{n-k}$$\n",
    "\n",
    "\n",
    "- Sampling $k$ items from $n$ items **with** replacement: $n^k$  \n",
    "- Sampling $k$ items from $n$ items **without** replacement: $n!/(n-k)!$ = $n(n-1)...(n-k+1)$\n",
    "- Partitions\n",
    "- Convention: $0! = 1$\n",
    "- Any **ordered** arrangement of objects is called a **permutation**. The number of different permutations of $N$ objects is $N!$. The number of different permutations of $n$ objects taken from $N$ objects is \n",
    "  \n",
    "  $$\\frac{N!}{(N-n)!}$$\n",
    "  \n",
    "  \n",
    "- Any **unordered** arrangement of objects is called a **combination**. The number of different combinations of $n$ objects taken from $N$ objects is \n",
    "  \n",
    "  $$\\frac{N!}{(N-n)!n!}$$ \n",
    "    \n",
    "  We typically denote this with $n \\choose N$ (\"N choose n\")\n",
    "\n",
    "- $$\\sum_{r=0}^k {m \\choose r}{n\\choose k-r} = {m+n \\choose k}$$\n",
    "\n",
    "<div class=\"alert alert-info\"><h3>Example</h3> <br> Which of the following identities expresses the probability of choosing from  m  fruits and  n  veggies a selection of  r  pieces of food, where exactly $k \\leq r$  of them are fruits and the rest are veggies? Answer: $$\\frac{\\binom{m}{k}\\binom{n}{r-k}}{\\binom{m+n}{r}}$$<br>There are  $\\binom{m}{k}\\binom{n}{r-k}$ ways to choose a set of $r$ foods with exactly $k$ fruits and the rest veggies. There are a total of $\\binom{m + n}{r}$ ways to choose a set of $r$ foods with no other restrictions. The probability is thus the former divided by the latter.</div>\n",
    "\n",
    "\n",
    "## Counting Useful Formulas\n",
    "\n",
    "Infinite Sum Law with $a$ positive and $\\leq 1$, i.e. geometric series\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\sum_{k=0}^{\\infty}a^{k}=\\frac{1}{1-a}}\n",
    "$$\n",
    "Starting from one:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\sum_{k=1}^{\\infty}a^{k}=\\frac{a}{1-a}}\n",
    "$$\n",
    "Starting from $j$:\n",
    "\n",
    "$$\n",
    "{\\displaystyle \\sum_{k=j}^{\\infty}2^{-k}=2^{-j+1}}\n",
    "$$\n",
    "Infinite Sum Law $b$:\n",
    "\n",
    "$$\n",
    "{\\displaystyle 0 + 1 + ... n = \\frac{n(n+1)}{2}}\n",
    "$$\n",
    "\n",
    "[Sequence](https://en.wikipedia.org/wiki/Geometric_series#Formula) / Sequence Convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint RVs\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions of RVs\n",
    "\n",
    "Let $X$  be a uniform random variable on $[0,1]$ and let  $Y=\\frac{1}{x}$.\n",
    "\n",
    "$$F_{Y}(y) = P(Y\\leq y) = P(\\frac{1}{X} \\leq y) = P(X\\geq \\frac{1}{y}) = 1 - \\frac{1}{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability Integral Transform\n",
    "\n",
    "Given any random continuous variable $X$, define $Y=F_{X}(X)$. Then:\n",
    "\n",
    "$$ \\begin{align}\n",
    "F_Y (y) &= \\operatorname{P}(Y\\leq y) \\\\\n",
    "        &= \\operatorname{P}(F_X (X)\\leq y) \\\\\n",
    "        &= \\operatorname{P}(X\\leq F^{-1}_X (y)) \\\\\n",
    "        &= F_X (F^{-1}_X (y)) \\\\\n",
    "        &= y\n",
    "\\end{align} $$\n",
    "\n",
    "$F_Y$ is just the CDF of a $\\text{Uniform}(0, 1)$ random variable. Thus, $Y$ has a uniform distribution on the interval $[0,1]$. As per the probability integral transformation, we need to be able to sample from  $U[0,1]$ , together with $F^{-1}_Y$, in order to do pseudorandom number generation of $Y$. Basically for each  $u \\in U[0,1]$ that we sample from $U[0,1]$, we calculate $y: F^{-1}_Y(u)$ as the corresponding sample from $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution\n",
    "\n",
    "$${\\displaystyle h(z)=(f*g)(z)=\\int _{-\\infty }^{\\infty }f(z-t)g(t)dt=\\int _{-\\infty }^{\\infty }f(t)g(z-t)dt}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order Statistics\n",
    "\n",
    "Let $Y_n = \\text{max}(X_1, . . . ,X_n)$. This is called the $n^{th}$ order statistic.\n",
    "\n",
    "How is the nth order statistic distributed (given independence): \n",
    "1. by definition of $Y_n$\n",
    "2. by independence\n",
    "\n",
    "\n",
    "$$F_n(y) = P(Y_n \\leq y) = P(X_1 \\leq y, X_2 \\leq y, ... , X_n \\leq y)\\\\\n",
    "P(X_1 \\leq y) P(X_2 \\leq y) \\dots P(X_n \\leq y)\\\\\n",
    "=F_X(y)^n$$\n",
    "\n",
    "First order stat\n",
    "\n",
    "$$P(X_1 \\geq y) P(X_2 \\geq y) \\dots P(X_n \\geq y)\\\\\n",
    "F_1(y) = (1 - F_X(n))^n\\\\\n",
    "f_1(y) = n(1 - F_X(n))^n f_X(y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### St. Petersburg Paradox\n",
    "\n",
    "$$E(Y)=\\sum_{n=1}^{\\infty} 2^n 0.5^{n-1}*0.5\\\\\n",
    "=\\sum_{n=1}^{\\infty} 2^n *(\\frac 12)^n\\\\\n",
    "= \\sum_{n=1}^{\\infty} 1\\\\\n",
    "=\\inf$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$E[Y] = E[X_{1}] * E[X_{2}]$$\n",
    "only if $X_1$ and $X_2$ are independent\n",
    "\n",
    "This is false. Unlike the case for expectations, the variance of a sum of random variables is equal to the sum of the variances of the random variables only when the random variables have zero covariance (which is always the case when they are independent).\n",
    "\n",
    "\n",
    "Law of Total Variance\n",
    "$${\\displaystyle \\operatorname {Var} (Y)=\\operatorname {E} [\\operatorname {Var} (Y\\mid X)]+\\operatorname {Var} (\\operatorname {E} [Y\\mid X]).}$$\n",
    "\n",
    "Law of Total Variance \n",
    "$${\\displaystyle \\operatorname {E} (X)=\\operatorname {E} (\\operatorname {E} (X\\mid Y)),}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimum Distribution\n",
    "\n",
    "If the cdf of $X_i$ is denoted by $F(x)$, then the cdf of the minimum is given by $1−(1−F(x))^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Rules\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)P(A)}{P(B)}}}$$\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B ∩ A)}{P(B)}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}